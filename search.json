[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nHi there, Welcome to Piyush.Dev\nI am a data scientist who enjoys playing with new technologies and identifying possible use cases in the real world.\nI am an engineer by qualification, turned to data science by sheer nature of curiosity.\nI like learning new things and sharing here.\nWelcome to my site.\nThis website is now powered by Quarto"
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "",
    "text": "I’ve been hacking on a RAG project for a few days, You know the one here: My RAG Guardrails App Repo"
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#building-on-giants-the-basic-rag-flow",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#building-on-giants-the-basic-rag-flow",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "Building on Giants (The Basic RAG Flow)",
    "text": "Building on Giants (The Basic RAG Flow)\nI didn’t reinvent basic RAG mechanics. Chunking, embedding, vector storage – that’s handled by leveraging great external tools like specialized vector DBs (Weaviate, Pinecone, etc.) and libraries (Langchain/LlamaIndex). My project focuses on orchestrating these at basic levels to demonstrate the functionality and adding layers on top."
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#nemo-guardrails-the-programmable-bouncer",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#nemo-guardrails-the-programmable-bouncer",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "Nemo Guardrails: The Programmable Bouncer",
    "text": "Nemo Guardrails: The Programmable Bouncer\nSo, NVIDIA’s Nemo Guardrails is pretty central here. How does it actually work? Think of it like programmable middleware sitting around the LLM calls.\nThe Magic: You define conversational “flows” in this language called Colang. Nemo intercepts the inputs (user query, retrieved context) and outputs (LLM response) and runs them through these flows.\nExample Checks: A flow might say:\n\n“Before calling the LLM, check the user input for toxic language using this specific classification model.”\nOr “After the LLM generates a response, ensure it doesn’t mention forbidden topics by scanning the text.”\n\nIt can even maintain state to check conversational history or ensure the LLM response aligns factually with the provided context using another LLM call. It’s powerful stuff for enforcing rules beyond simple prompt instructions.\n(And yeah, still didn’t focus much on super advanced context augmentation techniques – the main effort was elsewhere!)"
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#chatopenrouter-my-flexible-llm-gateway-battle-tested",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#chatopenrouter-my-flexible-llm-gateway-battle-tested",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "ChatOpenRouter: My Flexible LLM Gateway (Battle-Tested!)",
    "text": "ChatOpenRouter: My Flexible LLM Gateway (Battle-Tested!)\nThis custom class was born out of wanting flexibility. Why? To easily switch LLMs via services like OpenRouter.ai.\nUnder the Hood: Crucially, this class is built on Langchain’s base language model class. This foundation means it plays nicely with all sorts of other Langchain-based tools and libraries down the line. While the exact implementation can vary, my ChatOpenRouter class uses a library called as litellm (which is awesome for calling 100+ LLM APIs with a unified interface) and makes direct HTTP requests to the OpenRouter API endpoint. It also very easily handles mapping different provider model names (e.g., “openai/gpt-4” vs “anthropic/claude-3-opus”), manages API keys securely, and even have some basic retry logic baked in for when API calls inevitably hiccup.\nThe Stress Test: I did 3000+ API call binge over just 2-3 days. That wasn’t just random clicking! It involved rapidly iterating on prompts, testing different model responses via the router, debugging routing logic, and generally ensuring the abstraction didn’t add significant overhead or instability. It was crucial for validating that this flexible approach was actually practical."
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#ragas-grading-my-rags-homework-with-some-red-marks",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#ragas-grading-my-rags-homework-with-some-red-marks",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "Ragas: Grading My RAG’s Homework (With Some Red Marks!)",
    "text": "Ragas: Grading My RAG’s Homework (With Some Red Marks!)\nKnowing if the RAG output is good is key. Ragas helps quantify this.\nHow it Works (e.g., Faithfulness): Take the ‘Faithfulness’ metric. It’s pretty clever – Ragas often uses another LLM call under the hood! It typically breaks down the generated answer sentence by sentence. Then, for each sentence, it asks an LLM: “Can you verify this statement based only on the following retrieved text chunks?” It counts how many statements check out. It’s a neat way to approximate fact-checking against the context.\nThe Reality Check: Now, full disclosure time. Getting perfect scores across all Ragas metrics (Faithfulness, Answer Relevancy, Context Precision/Recall, etc.) is really tough, especially without ground-truth answers for everything. Right now, running the evaluations, I’ve definitely got some tests, particularly around metrics like Context Recall [or maybe another specific metric like Answer Correctness if applicable], that are still showing failures or scores lower than I’d like. It tells me there’s still tuning needed – maybe the retrieval isn’t pulling all the right info, or the prompt needs more tweaking to guide the LLM better. Work in progress!"
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#langfuse-my-debugging-crystal-ball",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#langfuse-my-debugging-crystal-ball",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "Langfuse: My Debugging Crystal Ball",
    "text": "Langfuse: My Debugging Crystal Ball\nDebugging RAG pipelines can feel like guesswork sometimes. Langfuse changes that.\nThe Integration: Getting it running was smoother than expected. You typically import the Langfuse SDK, maybe add a decorator (@observe()) to your key functions (like the main query handler, the retriever call, the LLM call via ChatOpenRouter), or use their context managers.\nWhat it Captures: It then asynchronously ships off tons of useful data for each run: the inputs/outputs of decorated functions, timings, metadata you add (like which model was used), the prompts, the retrieved chunks, LLM responses, even the calculated Ragas scores for that specific run if you integrate it! Looking at the trace in the Langfuse UI makes spotting the exact point of failure or bottleneck way easier than print() statements!"
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#testing-the-important-bits",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#testing-the-important-bits",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "Testing the Important Bits",
    "text": "Testing the Important Bits\nAnd yup, still got the standard tests: unit tests for small pieces, integration tests for the flow, and specific tests trying to fool my Nemo Guardrails."
  },
  {
    "objectID": "posts/2025-05-03-My-RAG-Misadventure.html#so-whats-the-real-point-here",
    "href": "posts/2025-05-03-My-RAG-Misadventure.html#so-whats-the-real-point-here",
    "title": "My RAG Misadventure: Guardrails, Open Routers, Ragas",
    "section": "So, What’s the Real Point Here?",
    "text": "So, What’s the Real Point Here?\nLook, let’s be clear: this project is not ‘finished’ software ready for primetime deployment. Like I mentioned, some of those Ragas evals highlight areas needing improvement!\nBut the main goal all along was to create a working demonstration of how to integrate these specific, powerful tools together in a RAG context:\n\nWiring up Nemo Guardrails for robust, programmable safety.\nBuilding and battle-testing a flexible LLM gateway like ChatOpenRouter.\nImplementing Ragas for serious, quantitative evaluation.\nAdding deep observability with Langfuse.\n\nIt’s really about showcasing that integration pattern. If you’re looking to build a RAG system that goes beyond the basics and incorporates these kinds of advanced features for safety, flexibility, evaluation, and observability, then hopefully, my messy, work-in-progress repo gives you a useful starting point or some concrete ideas.\nFeel free to dive into the code and see how the wires connect! Let me know if you have questions."
  },
  {
    "objectID": "posts/2025-05-07-Databricks-CLI-Github-Actions.html",
    "href": "posts/2025-05-07-Databricks-CLI-Github-Actions.html",
    "title": "Mysterious Databricks Bundle, DBT, GitHub Actions, and a Misleading Error",
    "section": "",
    "text": "You know how some days you just bang your head against the wall for hours over something that turns out to be deceptively simple? Yeah, that was me.\nI spent a whole day wrestling with a databricks.yml file. The mission, should I choose to accept it (and I did, albeit unknowingly at the start of the day), was to get our Databricks Asset Bundle – which was all nicely configured to build, run, and test our DBT models – to play nice with a GitHub Actions pipeline.\nWe’d already done our homework, or so I thought. The bundle was validated, we did a test deployment using the local Databricks CLI, and everything looked golden. The goal was CI/CD automation: push code, and let GitHub Actions handle the deployment to Databricks. Standard MLOps, data engineering goodness. You can find more about Databricks Asset Bundles here – they’re Databricks’ way of packaging up all your project files. And DBT (Data Build Tool), of course, is awesome for transforming data in your warehouse."
  },
  {
    "objectID": "posts/2025-05-07-Databricks-CLI-Github-Actions.html#the-moral-of-the-story",
    "href": "posts/2025-05-07-Databricks-CLI-Github-Actions.html#the-moral-of-the-story",
    "title": "Mysterious Databricks Bundle, DBT, GitHub Actions, and a Misleading Error",
    "section": "The Moral of the Story",
    "text": "The Moral of the Story\nHonestly, the only reason we burned an entire day on what should have been a straightforward YAML debugging session was because the error message sent us on a wild goose chase. The CLI was essentially saying, “I can’t create this job because your environment and environment keys are missing,” when the real issue was, “I don’t even know what a dbt is because the Python packages aren’t here!”\nThis whole ordeal really underscores a point: when you’re working with tools that are rapidly evolving or are wrappers around other tools (like the Databricks CLI orchestrating Terraform and interacting with DBT), sometimes the error messages aren’t as mature or direct as they could be. A production-ready tool, ideally, should give you error stack traces that point you closer to the actual root cause.\nSo, if you find yourself in a similar boat, scratching your head over a Databricks bundle deployment failing in CI/CD with weird Terraform-esque errors related to job definitions, especially when DBT is involved: double-check that your DBT dependencies are explicitly installed in your pipeline environment!\nHopefully, sharing this saves someone else the headache."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html",
    "title": "Intro to CVAT",
    "section": "",
    "text": "Once you start your journey into data science, you quickly learn that as part of your job you are spending more time with data than models. Data and labels go hand in hand and hence, I will be sharing what you need to know before you decide to use CVAT as your image/video annotation tool. Let’s start with the easier bit i.e. complete name of tool - Computer Vision Annotation Tool"
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#introduction",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#introduction",
    "title": "Intro to CVAT",
    "section": "",
    "text": "Once you start your journey into data science, you quickly learn that as part of your job you are spending more time with data than models. Data and labels go hand in hand and hence, I will be sharing what you need to know before you decide to use CVAT as your image/video annotation tool. Let’s start with the easier bit i.e. complete name of tool - Computer Vision Annotation Tool"
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-setup",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-setup",
    "title": "Intro to CVAT",
    "section": "Ease of Setup",
    "text": "Ease of Setup\nA treat awaits in this section if you have worked with docker compose before, CVAT is very easy to setup as the source code contains necessary docker compose files which makes complete setup a breeze in your local. Repo also features an instruction documentation which is comprehensively written and covers every line of code that needs to be executed in order to get the tool running.\nComplete setup is nicely segregated into docker-compose files with docker apps named cvat,cvat_ui, cvat_db. There are two ways to go ahead with setting up the tool, the easier way of deploying it in a VM and a little longer way on Kubernetes. A VM with enough firepower to run an instance of Postgres, Django backend and a react app to support the number of users that you expect to access the tool simultaneously should be alright. An example: I was able to support 200+ users(some of them were automated scripts) in a single VM with the analytics component running using a Standard D5 v2 (16 vcpus, 56 GiB memory) Azure VM. The power of your VM needs to be proportionately increased if you wish to use the additional components like deep learning model based auto labelling. If you choose to deploy the tool in Kubernetes, you could benefit from the auto-scaling functionality for each app inside CVAT. Kubernetes YAML files are not available as part of source code and hence you might need to create them yourself. I recommend Kompose to create Kubrnetes YAML from docker compose files. Scalability is an important aspect while considering a tool for production deployment. In order to make sure that the tool works for foreseeable growth in number of users. App can be scaled easily since everything is nicely wrapped into docker containers."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-usage",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-usage",
    "title": "Intro to CVAT",
    "section": "Ease of Usage",
    "text": "Ease of Usage\nThe application’s desktop UI although not fancy, is very feature rich and achieves the goal of labeling images and videos with ease.There’s a long list of keyboard shortcuts supported and you don’t necessarily need to remember every shortcut, simply pick and choose which help you speed up. I found the shortcut to create poly shapes and rectangles during labelling as very useful. Keyboard shortcuts combined with the feature rich app make labelling task slick and smooth. The application also features a task assignment and task process flow using which larger teams can collaborate by assigning tasks to a specific user and updating the current status of task for others to see. CVAT is developed considering the desktop based user interface, which means we need to keep expectations lower while trying to use it on mobile or tablets. You can try the app online right now by navigating to cvat.org CVAT also features a command line interface which enables you to perform simple CRUD operations on task."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#data-extraction-upload",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#data-extraction-upload",
    "title": "Intro to CVAT",
    "section": "Data Extraction/ Upload",
    "text": "Data Extraction/ Upload\nA Django app in backend and react UI as frontend, CVAT is quite covered with options to upload data via UI or CLI. App works on a unique task based system, each upload is created as a task in the system. The task can then be further assigned to different users for labelling, quality check, etc. When using the UI to upload data, you don’t need to worry about label formatting because the app takes care of it. CLI based upload requires data to be structured in specific formats before it can be processed. I recommend CLI to perform automated scripts based data upload and UI when actual human is performing the uplaod and labelling task.\nExtraction of data is a very important step which will have to be performed by every team on regular basis. CVAT supports extraction of data in a format by both interfaces i.e. UI and CLI. A user can go to a task and there’s option to extract the data in various supported formats. when trying to extract multiple data points or tasks in CVAT, UI based extraction might seem time consuming . CLI comes to rescue here, extraction of data in any supported format is super simple using CLI. An important thing to note here is CVAT currently doesn’t support bulk extraction or upload of data using UI.\nThe dev team also mentiond about datumaro dataset framework which can be used to transform, merge, extract multiple datasets from CVAT. I was not able to get it working and therefore no comments on that."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#annotations-format-supported",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#annotations-format-supported",
    "title": "Intro to CVAT",
    "section": "Annotations Format Supported",
    "text": "Annotations Format Supported\nI am borrowing a table available in CVAT documentation to show the formats supported. It supports all major community defined data label formats. The labels covers the spectrum of classification, obejct detection and segmentation tasks in computer vision.\n\n\n\nAnnotation format\nImport\nExport\n\n\n\n\nCVAT for images\nX\nX\n\n\nCVAT for a video\nX\nX\n\n\nDatumaro\n\nX\n\n\nPASCAL VOC\nX\nX\n\n\nSegmentation masks from PASCAL VOC\nX\nX\n\n\nYOLO\nX\nX\n\n\nMS COCO Object Detection\nX\nX\n\n\nTFrecord\nX\nX\n\n\nMOT\nX\nX\n\n\nLabelMe 3.0\nX\nX"
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#backup-and-restore",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#backup-and-restore",
    "title": "Intro to CVAT",
    "section": "Backup and Restore",
    "text": "Backup and Restore\nAll images/videos uploaded are stored in docker volume cvat_data and the respective label data is stored in postgres. Postgres data is stored in docker volume cvat_db. In order to backup the complete app data, you can simply create volume backup for these two volumes in form of .tar files. Configuring the said docker volumes to a persistent storage like S3 or azure blob would enable you to setup automated cloud backup for these volumes. Restoration is as simple as backup by using docker commands."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#community",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#community",
    "title": "Intro to CVAT",
    "section": "Community",
    "text": "Community\nCVAT community is available on GitHub and Gitter. I have personally found them responding faster on gitter compared to raising issues on github."
  },
  {
    "objectID": "posts/inertia/inertia.html",
    "href": "posts/inertia/inertia.html",
    "title": "The Inertia Model and Its Relevance to AI Models",
    "section": "",
    "text": "The Inertia Model, as described in The Great Mental Models series, explains how objects—or systems—tend to resist change due to existing momentum or stagnation. In physics, inertia keeps a moving object in motion unless acted upon by an external force. The same principle applies to decision-making, businesses, and even artificial intelligence models.\nAI models, once trained, exhibit a form of inertia. They rely on the data they were trained on and the assumptions built into their architectures. Without intentional updates or external intervention, they continue making predictions based on historical patterns, even when those patterns become outdated. For example, a recommendation algorithm trained on past user behavior may struggle to adapt to sudden shifts in preferences unless it is retrained with fresh data. Another example of inertia, traditional businesses like banking and insurance continue to struggle in adopting latest AI technologies in their core areas.\nAnother aspect of inertia in AI is model bias. If an AI system has learned biased associations from historical data, that bias will persist unless corrective measures—such as better data curation or active bias mitigation—are introduced. The resistance to change mirrors human cognitive inertia, where existing beliefs and habits are difficult to alter without strong external stimuli.\nApplications and Decision making, especially in relation to AI, chimes well with this model. AI models/applications that are incrementally increasing/improving certain parts of a business process are leveraging existing momentum. Comparatively easier to explain the significance, value and cost to the business. It aligns well with the direction and movement of the organization and finds easier/wider acceptance in the organisation. Consider an AI model/application that is fundamentally disrupting the existing process or probably making it altogether redundant; they require significantly more escape velocity (so to speak, in cost, explanation, value) to achieve the desired results and hardly see the light of the day.\nIn a broader sense, recognizing inertia in AI systems helps researchers and practitioners understand the importance of continuous learning and adaptation. Just as overcoming inertia in human behavior requires conscious effort, ensuring AI models remain effective and relevant requires ongoing intervention, retraining, and validation.\nBy applying the Inertia Model, we can better anticipate challenges in AI development and deployment, ensuring that models do not stagnate or reinforce outdated patterns, but instead evolve with the real-world data they interact with."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html",
    "href": "posts/devops/Deploying-RL-Model.html",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "",
    "text": "Our team is focused on enhancing user engagement and expanding our reach to a larger audience. One of the most effective ways to achieve this is through a well-designed mobile application, which already has millions of downloads.\nWithin the app, there is a dynamic carousel on the home page that displays banners. These banners serve multiple purposes, including providing information, facilitating communication, and highlighting key features of the app. Since this is the first page users see after registering, it plays a critical role in shaping their experience and guiding their interactions. A portion of users engage with these banners, while others explore the app through different paths.\nOur goal is to better understand user engagement patterns—identifying what drives some users to interact with banners while others take different routes. By analyzing these behaviors, we aim to refine the user experience and optimize engagement within the app."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#introduction",
    "href": "posts/devops/Deploying-RL-Model.html#introduction",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "",
    "text": "Our team is focused on enhancing user engagement and expanding our reach to a larger audience. One of the most effective ways to achieve this is through a well-designed mobile application, which already has millions of downloads.\nWithin the app, there is a dynamic carousel on the home page that displays banners. These banners serve multiple purposes, including providing information, facilitating communication, and highlighting key features of the app. Since this is the first page users see after registering, it plays a critical role in shaping their experience and guiding their interactions. A portion of users engage with these banners, while others explore the app through different paths.\nOur goal is to better understand user engagement patterns—identifying what drives some users to interact with banners while others take different routes. By analyzing these behaviors, we aim to refine the user experience and optimize engagement within the app."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#problem-statement",
    "href": "posts/devops/Deploying-RL-Model.html#problem-statement",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "Problem statement",
    "text": "Problem statement\nThe goal was to increase user engagement within the app by understanding user’s interest in a variety of banners and then leverage the results across the app.\nwe didn’t have existing data about user interaction with the app neither did we have enough time at hand to perform that activity. We were also looking at an incoming huge inflow of new users expected in near future due to the planned marketing campaigns. We were essentially looking at a cold start problem to improve engagement since, we would know little about the new users and time to market was a very important factor. We were expected to go live within two weeks duration with a solution to make the best out of data available at hand."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#solution",
    "href": "posts/devops/Deploying-RL-Model.html#solution",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "Solution",
    "text": "Solution\nBayesian bandits with Thomson sampling ticked all boxes as follows: 1. It requires no data or less to start with compared to other options 2. It will learn incoming users/data and start recommending banners 3. Can work with new banners configured as new arms\nThe next phase of the project was also discussed where we agreed to work on building contextual bandits. In this post, I will be talking more about how we used various tools and technology making deployment possible. I will not be talking about how the recommendation algorithm works and the technology stack used to achieve it."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#deployment",
    "href": "posts/devops/Deploying-RL-Model.html#deployment",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "Deployment",
    "text": "Deployment\n\nThe build and deployment part of the project was broken down into two technical stages/phases: 1. Testing model and documenting results in the pre-prod environment with production data, define the input and the output schema for the model which will be used by the data engineer team to create a streaming pipeline. 2. Setup model to consume a live stream of event data, and respond via a REST endpoint with the recommended list of banners for the users\nThe front end of the mobile app is configured for a response time of one second w.r.t to back-end. It meant that the app will try to generate dynamic banners on the user screen based on our recommendations or fall back to static banners if we failed to deliver a response within a second, which added another layer of complexity to the second stage. Our APIs were expected to support a wide range of user load starting from a few hundred requests to millions across the region.\nWe could list the deployment infra into three major components: 1. A robust build and deployment pipeline 2. Automated performance testing 3. Production monitoring and alerting"
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#tools",
    "href": "posts/devops/Deploying-RL-Model.html#tools",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "Tools",
    "text": "Tools\nTools used for the complete setup: 1. Jenkins 2. Artifactory 3. Docker 4. Aquasec image scanning 5. Fortify static code scan 6. Sonar Nexus open source code scanning 7. Kubernetes 8. Predator 9. Prometheus 10. Grafana 11. Bitbucket\nOur application solution is a bunch of docker images which consumes/produces content in Kafka topics.\n\nStep 1 — Fetching code and checking for changes\nOur pipeline starts at fetching the code from Bitbucket repository. We store code in the folder structure for the 4 different docker images that are to be built. We check whether a file has been changed before initiating build for the files in that folder. The code in the Jenkins pipeline is as below for one of the folders titled ‘generator’.\n//hide_output\nscript{\n    GIT_RESULT = sh(script: '''git diff --quiet HEAD \\\"$(git rev-parse @~1)\\\" -- generator''',\n        returnStatus: true\n        )\n        echo \\\"GIT RESULT -- ${GIT_RESULT} -- ${params.branchname}\\\"\n}\n\n\nStep 2 - Fortify\nNext step is to run complete code static security scanning by Fortify\n//hide_output\nsh '''\n  echo \\\"==================================================\\\"\n  echo \\\"========--- SAST - Fortify Scan: Start ---========\\\"\n  echo \\\"==================================================\\\"\n  hostname\n  whoami\n  ls -ahl\n  echo 'WORKSPACE: ' $WORKSPACE\n  cd $WORKSPACE\n  pwd\n  sourceanalyzer -v\n  sourceanalyzer -b ${fortify_app_name} -clean\n  sourceanalyzer -b ${fortify_app_name} -python-version ${python_version} -python-path ${python_path} ${fortify_scan_files}\n  sourceanalyzer -b ${fortify_app_name} -scan -f ${fortify_app_name}.fpr\n  fortifyclient -url https://sast.intranet.asia/ssc -authtoken \\\"${fortify_upload_token}\\\" uploadFPR -file ${fortify_app_name}.fpr -project ${fortify_app_name} -version ${fortify_app_version}\n'''\n\n\nStep 3 - Docker\nThe next step is to build the docker image. We first login to Artifactory before initiating the build as our pip libraries are also pulled from mirrored pip in the Artifactory. I have provided a sample of code on how we achieve this.\n//hide_output\nsh \"\"\"\n    echo ${ARTIFACTORY_PASSWORD} | docker login -u ${ARTIFACTORY_USERNAME} --password-stdin docker-registry:8443\n    cd generator\n    docker build --file Docker-dev \\\\\n     --build-arg HTTPS_PROXY=http://ip-address \\\\\n     --build-arg ARTIFACTORY_USERNAME=${ARTIFACTORY_USERNAME} \\\\\n     --build-arg ARTIFACTORY_PASSWORD=${ARTIFACTORY_PASSWORD} \\\\\n     -t ${env.generator_image_latest} .\n    docker tag ${env.generator_image_latest} ${env.generator_image_name}\n    docker push ${env.generator_image_latest}\n    docker push ${env.generator_image_name}\n    docker logout docker-repo-path\n    cd ..\n\"\"\"\n\n\nStep 4 - Aquasec\nAfter pushing an image into Artifactory, the next important and mandatory step to be performed is docker image security scanning.\n//hide_output\nsh \"\"\"\n     echo \\\"==================================================\\\"\n     echo \\\"=============--- OSS - Nexus Scan ---=============\\\"\n     echo \\\"==================================================\\\"\n            docker save -o generator-dev.tar ${env.generator_image_latest}\n            \"\"\"\n            String result = nexusscan(\\\"pcaaicoeaipulsenudgesgeneratordev\\\", \\\"$WORKSPACE\\\", \\\"build\\\");\n            echo result;\n            sh \"\"\"\n            rm -f generator-dev.tar\n            \"\"\"\n            sh \"\"\"\n            echo \\\"==================================================\\\"\n            echo \\\"=============--- CSEC - Aquasec Scan ---==========\\\"\n            echo \\\"==================================================\\\"\n\"\"\"\n            aquasecscan(\\\"${env.generator_image_latest}\\\")\nThe code and image security scanning stages are major milestones to be cleared during the deployment phase. It is important as well as difficult to explain and agree between application security teams about what risks are we willing to take while allowing open source libraries with bugs to go live in our environment.\n\n\nStep 5 — Kubernetes\nNow we move on to the stage where we will be able to actually deploy and run our images. In order to deploy our solution, we need a Redis DB and Kafka cluster up and running. We deploy our docker images using the below code:\n//hide_output\nsh '''\n    set +x\n    echo \\\"---- preparing options ----\\\"\n    export HTTPS_PROXY=ip-address:8080\n    export KUBE_NAMESPACE=\\\"internal-namespace\\\"\n    export KC_OPTS=${KC_OPTS}\\\" --kubeconfig=${KUBE_CONFIG}\\\"\n    export KC_OPTS=${KC_OPTS}\\\" --insecure-skip-tls-verify=true\\\"\n    export KC_OPTS=${KC_OPTS}\\\" --namespace=${KUBE_NAMESPACE}\\\"\n    \n    echo \\\"---- prepared options ----\\\"\n    echo \\\"---- preparing alias ----\\\"\n    alias kc=\\\"kubectl ${KC_OPTS} $*\\\"\n    echo \\\"---- alias prepared ----\\\"\n    \n    echo \\\"---- applying manifest ----\\\"\n   \n   kc apply -f configmap.yaml\n\n   if [ $which_app = \\\"generator\\\" ];then\n     if [ $image_version = \\\"latest\\\" ];then\n       kc delete deploy ai-pulse-nudges-events-reader||echo\n     fi\n     sed -i \\\"s!GENERATOR_VERSION!$image_version!g\\\" \\\"generator.yaml\\\"\n     kc apply -f generator.yaml\n   fi  \n\n   if [ $which_app = \\\"aggregator\\\" ];then\n     if [ $image_version = \\\"latest\\\" ];then\n       kc delete deploy ai-pulse-nudges-click-counter||echo\n     fi\n     sed -i \\\"s!AGGREGATOR_VERSION!$image_version!g\\\" \\\"aggregator.yaml\\\"\n     kc apply -f aggregator.yaml\n   fi\n\n   if [ $which_app = \\\"detector\\\" ];then\n     if [ $image_version = \\\"latest\\\" ];then\n       kc delete deploy ai-pulse-nudges-engine||echo\n     fi\n     sed -i \\\"s!DETECTOR_VERSION!$image_version!g\\\" \\\"detector.yaml\\\" \n     kc apply -f detector.yaml\n   fi\n\n   if [ $which_app = \\\"restapi\\\" ];then\n     if [ $image_version = \\\"latest\\\" ];then\n       kc delete deploy ai-pulse-nudges-restapi||echo\n        fi\n     sed -i \\\"s!REST_VERSION!$image_version!g\\\" \\\"restapi.yaml\\\"\n     kc apply -f restapi.yaml\n            fi\n    if [ $which_app = \\\"all\\\" ];then\n     if [ $image_version = \\\"latest\\\" ];then\n     kc delete deploy ai-pulse-nudges-events-reader||echo\n     kc delete deploy ai-pulse-nudges-click-counter||echo\n     kc delete deploy ai-pulse-nudges-engine||echo\n     kc delete deploy ai-pulse-nudges-restapi||echo\n     fi\n\n     sed -i \\\"s!GENERATOR_VERSION!$image_version!g\\\" \\\"generator.yaml\\\"\n     sed -i \\\"s!AGGREGATOR_VERSION!$image_version!g\\\" \\\"aggregator.yaml\\\"\n     sed -i \\\"s!DETECTOR_VERSION!$image_version!g\\\" \\\"detector.yaml\\\" \n     sed -i \\\"s!REST_VERSION!$image_version!g\\\" \\\"restapi.yaml\\\"\n\n     kc apply -f generator.yaml\n     kc apply -f aggregator.yaml\n     kc apply -f detector.yaml\n     kc apply -f restapi.yaml\n            fi\n   \n   \n   \n    echo \\\"---- manifest applied ----\\\"\n    echo \\\"---- checking result ----\\\"\n    \n    echo \\\" &gt;&gt; Deployments \\\"\n    kc get deployments\n    \n    echo \\\" &gt;&gt; Services\\\"\n    kc get svc\n    \n    echo \\\" &gt;&gt; Ingress\\\"\n    kc get ingress\n    \n    echo \\\" &gt;&gt; Pods\\\"\n    kc get pods\n    \n    echo \\\"---- Done ----\\\"\n'''\n\n\nStep 6 — Performance test\nWe deploy Predator — the tool which we use for performance test.\n//hide_output\nsh '''\n    set +x\n    echo \\\"---- preparing options ----\\\"\n    export HTTPS_PROXY=ip-address:8080\n    export KUBE_NAMESPACE=\\\"internal-namespace\\\"\n    export KC_OPTS=${KC_OPTS}\\\" --kubeconfig=${KUBE_CONFIG}\\\"\n    export KC_OPTS=${KC_OPTS}\\\" --insecure-skip-tls-verify=true\\\"\n    export KC_OPTS=${KC_OPTS}\\\" --namespace=${KUBE_NAMESPACE}\\\"\n    \n    echo \\\"---- prepared options ----\\\"\n    echo \\\"---- preparing alias ----\\\"\n    alias kc=\\\"kubectl ${KC_OPTS} $*\\\"\n    echo \\\"---- alias prepared ----\\\"\n    \n    echo \\\"---- applying manifest ----\\\"\n   \n   kc get deploy|grep predator|awk '{print $1 }' || echo\n   kc get deploy|grep predator|awk '{print $1 }'|xargs kc delete deploy || echo\n\n   for i in `seq $replica_count`\n   do\n     echo $i\n     cp -rf predator/predator.yaml tmp.yaml\n     sed -i \\\"s!REPLICA_NO!$i\\\"\\\"!g\\\" \\\"tmp.yaml\\\"\n     kc apply -f tmp.yaml\n   done  \n   \n   \n    # kc apply -f predator/predator.yaml\n    \n    echo \\\"---- manifest applied ----\\\"\n    echo \\\"---- checking result ----\\\"\n    \n    echo \\\" &gt;&gt; Deployments \\\"\n    kc get deployments\n    \n    echo \\\" &gt;&gt; Services\\\"\n    kc get svc\n    \n    echo \\\" &gt;&gt; Ingress\\\"\n    kc get ingress\n    \n    echo \\\" &gt;&gt; Pods\\\"\n    kc get pods\n    \n    echo \\\"---- Done ----\\\"\n'''\nPredator is an amazing tool that enables us to leverage existing Kubernetes infra for an unlimited number of users for testing. Read more about the tool here: https://medium.com/zooz-engineering/by-niv-lipetz-software-engineer-zooz-b5928da0b7a8 We leverage the existing enterprise Prometheus and Grafana set up to monitor the application pods."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#lessons-learned-for-next-time",
    "href": "posts/devops/Deploying-RL-Model.html#lessons-learned-for-next-time",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "Lessons learned for next time:",
    "text": "Lessons learned for next time:\n\nWe started writing the pipeline code from scratch, whereas it would have helped save time if an advanced hello world type of empty pipeline existed, which could be used as a template structure. It would have enabled us to know what credentials and access were required at what stage.\nThere were many credentials and access that were required to get the pipeline up and running. It would be a time and effort savior if we have one master service id created and assigned to a pipeline which can then be used across all tools in the organization.\nIt is very difficult to build a machine learning model, and real-time streaming data was an additional complexity, but productionizing that model with streaming data is many folds difficult."
  },
  {
    "objectID": "posts/devops/Deploying-RL-Model.html#contributors",
    "href": "posts/devops/Deploying-RL-Model.html#contributors",
    "title": "Deployment Journey of a Reinforcement Learning Algorithm",
    "section": "Contributors",
    "text": "Contributors\nGlenn Bayne, Tien Nguyet Long, John Yue, Zeldon Tay （郑育忠), Steven Chau , Denys Pang , Philipp Gschoepf , Syam Bandi , Uma Maheshwari, Michael Natusch"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html",
    "title": "Random Convolutional Kernel Transform",
    "section": "",
    "text": "ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels is a research paper published in October 2019 by Angus Dempster, François Petitjean, Geoffrey I. Webb. The paper presents a unique methodology to transform time series data using convolutional kernels to improve classification accuracy. This paper is unique in learning from the recent success of convolutional neural networks and transferring it on time-series datasets.\nThe link to download the paper from arxiv - Paper"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#introduction",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#introduction",
    "title": "Random Convolutional Kernel Transform",
    "section": "",
    "text": "ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels is a research paper published in October 2019 by Angus Dempster, François Petitjean, Geoffrey I. Webb. The paper presents a unique methodology to transform time series data using convolutional kernels to improve classification accuracy. This paper is unique in learning from the recent success of convolutional neural networks and transferring it on time-series datasets.\nThe link to download the paper from arxiv - Paper"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#time-series-data",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#time-series-data",
    "title": "Random Convolutional Kernel Transform",
    "section": "Time Series data",
    "text": "Time Series data\nTime series data is defined as a set of data points containing details about different points in time. Generally, time-series data contains data points sampled/observed at an equal interval of time. Time series classification is task of identifying patterns and signals in the data in relation to respective classes.\nThe proposal is features generated by the convolution of randomly generated kernels on time series data results in faster and better time series classifiers. We will go into more detail of this proposal and understand how the methodology proposed by them helps to improve the accuracy."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#kernels",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#kernels",
    "title": "Random Convolutional Kernel Transform",
    "section": "Kernels",
    "text": "Kernels\nKernels in simple terms is a small matrix used to modify the images. Let’s try to understand kernels using an example:\nhere is a 3 x 3 kernel used to sharpen images:\n\\[\n\\\\begin{bmatrix} 0 & -1 & 0 \\\\\\\\ -1 & 5 & -1 \\\\\\\\ 0 & -1 & 0  \\\\end{bmatrix}\n\\]\nTo sharpen an image using the above kernel, we need to perform a dot product of each pixel in the image with the kernel matrix. The resulting image would then be a sharpened version of the original image. Observe the gif below to see a live version of the kernel dot product in motion.\nFollowing is an example from setosa.io site to demonstrate how kernels can be used to make desirable changes to an image.\n\n\n5 parameters of kernels\nA kernel has 5 different parameter using which it can be configured.\n\n\n\n\n\n\n\n\nParameter\nDescription\nValue logic\n\n\n\n\nBias\nBias is added to the result of the convolution operation between input time series and weights of the given kernel\nBias is sampled from a uniform distribution, b ∼ U(−1,1)\n\n\nSize(Length)\nSize defines the number of rows and columns a kernel has. The above example has a size of 3 rows and 3 columns\nLength is selected randomly from {7,9,11} with equal probability, making kernels considerably shorter than input time series in most cases\n\n\nWeights\nThe values that make up the kernel matrix are weights\nThe weights are sampled from a normal distribution, ∀w ∈ W, w ∼ N(0,1), and are mean-centered after being set, ω = W − W. As such, most weights are relatively small, but can take on larger magnitudes\n\n\nDilation\nDilation spreads a kernel over the input such that with dilation of value two, weights in a kernel are convolved with every second element of input time series\nDilation is sampled on an exponential scale d = ⌊2x⌋,x ∼ U(0,A), input −1 where A = log2 kernel −1\n\n\nPadding\nPadding involves appending values(typically zero) to the start and end of input time series such that the middleweight of a kernel aligns with the first value of input time series at the start of convolution\nWhen each kernel is generated, a decision is made (at random, with equal probability) whether or not padding will be used when applying the kernel"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#features-generated-by-rocket-kernel",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#features-generated-by-rocket-kernel",
    "title": "Random Convolutional Kernel Transform",
    "section": "Features generated by Rocket kernel",
    "text": "Features generated by Rocket kernel\nRocket computes two aggregate features from each kernel and feature convolution. The two features are created using the well-known methodology global/average max pooling and a unique methodology positive proportion value (ppv).\n\nMax-pooling\nGlobal max-pooling is essentially picking the maximum value from the result of convolution and max pooling is picking the maximum value within a pool size. Assuming that the output of convolution is 0,1,2,2,5,1,2, global max pooling outputs 5, whereas ordinary max pooling with pool size equals to 3 outputs 2,2,5,5,5\n\n\nProportion of positive values\nLet’s try to understand using the author’s own words to describe ppv.\n\nppv directly captures the proportion of the input which matches a given pattern, i.e., for which the output of the convolution operation is positive. The ppv works in conjunction with the bias term. The bias term acts as a kind of ‘threshold’ for ppv. A positive bias value means that ppv captures the proportion of the input reflecting even ‘weak’ matches between the input and a given pattern, while a negative bias value means that ppv only captures the proportion of the input reflecting ‘strong’ matches between the input and the given pattern."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-usage",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-usage",
    "title": "Random Convolutional Kernel Transform",
    "section": "Rocket usage",
    "text": "Rocket usage\nNow that we understand what kernels are and how rocket generates two outputs by convolution of kernel and input vector, let’s understand how to use it.\nThe time-series data needs to be provided as input into the rocket transform method, the value for the number of kernels (i.e. k) is set at 10,000 by default. This means that for each one of the input features we get 20,000 features as output from rocket transform.\nThe transformed feature table can now we used as input data for any classification algorithm, authors advise linear algorithms like ridge regression classifier or logistic regression."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-vs-others",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-vs-others",
    "title": "Random Convolutional Kernel Transform",
    "section": "Rocket v/s others",
    "text": "Rocket v/s others\nRocket’s approach of creating a large number of random kernels and generating two features from each kernel is unique. Rocket distinguishes itself based on various other factors which we will discuss below.\n\nRocket v/s neural nets\n\nRocket doesn’t use a hidden layer or any non-linearities\nFeatures produced by Rocket are independent of each other\nRocket works with any kind of classifier\n\n\n\nRocket v/s CNN\n\nRocket uses a very large number of kernels\nIn CNN, a group of kernels tend to share the same size, dilation and padding. Rocket has all 5 parameters randomized.\nIn CNN, Dilation increases exponentially with depth; Rocket has random dilation values\nCNNs only have average/max pooling. Rocket has a unique pooling called as ppv which has proven to provide much better classification accuracy on time series."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-performance",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-performance",
    "title": "Random Convolutional Kernel Transform",
    "section": "Rocket performance",
    "text": "Rocket performance\nThe authors provide detailed information about the classification accuracy and time taken to train the model. I am discussing the results from bakeoff datasets in this article and you will be able to find results from various additional datasets in the paper.\n\nAccuracy\nRank is calculated by taking a mean value of classification accuracy across all the 85 datasets in bakeoff datasets.\nIt is clear that the model trained using features derived using rocket is faring better compared to other models on average among all the datasets in bake-off datasets. Please note that the dark horizontal line connecting the rank position of two models depict that the results from two models are not statistically insignificant.\n\n\n\nTime taken to train\n\n\n\n\n\n\n\n\nArchitecture\nLargest dataset(ElectricDevices, with 8,926 training examples)\nLongest time series(HandOutlines, with time series of length 2,709)\n\n\n\n\nROCKET\n6 minutes 33 seconds\n4 minutes 18 seconds\n\n\nMrSEQL\n31 minutes\n1 hour 55 minutes\n\n\ncBOSS\n3 hours 6 minutes\n42 minutes\n\n\nProximity Forest\n1 hour 35 minutes\n3 days\n\n\nTS-CHIEF\n2 hours 24 minutes\n4 days\n\n\nInception Time (on GPU)\n7 hours 46 minutes\n8 hours 10 minutes"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#example",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#example",
    "title": "Random Convolutional Kernel Transform",
    "section": "Example",
    "text": "Example\nIn the below examples, we are going to try and train a Human activity recognizer time series classifier. I am using two nice repo by Guillaume Chevalier showcasing LSTM model on Human activity recognizer with a classification accuracy of 91% and by Thanatchon . Let’s see how much accuracy can be achieved by using rocket transforms.\nWe will be using sktime implementation of rocket in this example\n\nInstall libraries & import Statements\n#hide\n! pip install pandas numba scikit-learn sktime matplotlib\n#collapse-hide\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport os\nimport zipfile\n#orchestration\nfrom sklearn.pipeline import make_pipeline\n\n#transforms\nfrom sktime.transformers.series_as_features.rocket import Rocket \n\n#plot\nimport matplotlib.pyplot as plt\n\n#metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Classifier\nfrom sklearn.linear_model import RidgeClassifierCV\n\n\nDownload dataset and extract\n#collapse-hide\n# Download the file\n!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"\n#collapse-hide\n# Extract\n\nzip_ref = zipfile.ZipFile('UCI HAR Dataset.zip', 'r')\nzip_ref.extractall('./')\nzip_ref.close()\n#collapse-hide\n# validate if file exists\n!ls ./'UCI HAR Dataset'\n#collapse-hide\n# Useful Constants\n\n# Those are separate normalised input features for the neural network\nINPUT_SIGNAL_TYPES = [\n    \"body_acc_x_\",\n    \"body_acc_y_\",\n    \"body_acc_z_\",\n    \"body_gyro_x_\",\n    \"body_gyro_y_\",\n    \"body_gyro_z_\",\n    \"total_acc_x_\",\n    \"total_acc_y_\",\n    \"total_acc_z_\"\n]\n\n# Output classes to learn how to classify\nLABELS = [\n    \"WALKING\", \n    \"WALKING_UPSTAIRS\", \n    \"WALKING_DOWNSTAIRS\", \n    \"SITTING\", \n    \"STANDING\", \n    \"LAYING\"\n]\n\n\nDefining Train test data\n#collapse-show\nDATA_PATH = \"./\"\nTRAIN = \"train/\"\nTEST = \"test/\"\nDATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n\n\n# Load \"X\" (the neural network's training and testing inputs)\n\ndef load_X(X_signals_paths):\n    X_signals = []\n    \n    for signal_type_path in X_signals_paths:\n        file = open(signal_type_path, 'r')\n        # Read dataset from disk, dealing with text files' syntax\n        X_signals.append(\n            [np.array(serie, dtype=np.float32) for serie in [\n                row.replace('  ', ' ').strip().split(' ') for row in file\n            ]]\n        )\n        file.close()\n    \n    return np.transpose(np.array(X_signals), (1, 2, 0))\n\nX_train_signals_paths = [\n    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \n    \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n]\nX_test_signals_paths = [\n    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \n    \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n]\n\nX_train = load_X(X_train_signals_paths)\nX_test = load_X(X_test_signals_paths)\n\n\n# Load \"y\" (the neural network's training and testing outputs)\n\ndef load_y(y_path):\n    file = open(y_path, 'r')\n    # Read dataset from disk, dealing with text file's syntax\n    y_ = np.array(\n        [elem for elem in [\n            row.replace('  ', ' ').strip().split(' ') for row in file\n        ]], \n        dtype=np.int32\n    )\n    file.close()\n    \n    # Substract 1 to each output class for friendly 0-based indexing \n    return y_ - 1\n\ny_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\ny_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n\ny_train = load_y(y_train_path)\ny_test = load_y(y_test_path)\n\n\nprint('OK !')\n\n\nPreparing dataset\n#collapse-show\ndef preprocess_data(data_array):\n\n    dim_dict = {}\n\n    for i in range(9):\n        name_i = f'dim_{str(i)}'\n        dim_dict[name_i] = []\n\n    for i in range(data_array.shape[0]):\n        for j in range(data_array.shape[1]):\n            name_dim = f'dim_{str(j)}'\n            dim_dict[name_dim].append(\n                pd.Series(data_array[i][j]).astype('float64'))\n            \n    return pd.DataFrame(dim_dict)\n\nprint('OK !')\n#collapse-show\nX_train = X_train.reshape(-1,9,128)\nX_test = X_test.reshape(-1,9,128)\n\n%time X_train = preprocess_data(X_train)\n%time X_test = preprocess_data(X_test)\n\ny_train = list(y_train.copy().ravel())\ny_test = list(y_test.copy().ravel())\n\ny_train = [str(each) for each in y_train]\ny_test = [str(each) for each in y_test]\n\n\nRocket Model\nrocket_pipeline = make_pipeline(\n    Rocket(num_kernels = 10000, random_state = 1),\n    RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n)\n\n%time rocket_pipeline.fit(X_train, y_train)\n\n\nAccuracy\n%time rocket_pipeline.score(X_test, y_test)\n%time y_pred = list(rocket_pipeline.predict(X_test))\n%time plot_confusion_matrix(rocket_pipeline, X_test, y_test, display_labels = LABELS)"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#conclusion",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#conclusion",
    "title": "Random Convolutional Kernel Transform",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to improve upon accuracy achieved by LSTM model by a very simple implementation using Rocket. LSTM had scored 91% and using Rocket + Ridge Regression classifier the accuracy jumped to 93%. The performance gain achieved becomes sweeter when you compare the time required to code and train, which was very small compared for rocket in comparison to LSTM in our case. The Rocket methodology is an innovative, simple and fresh technique that attracted my attention to this research paper."
  },
  {
    "objectID": "posts/2025-04-28-My-Weekend-Wrestling-Match-with-Madrigal.html#my-weekend-wrestling-match-with-madrigal-a-mlops-deployment-story",
    "href": "posts/2025-04-28-My-Weekend-Wrestling-Match-with-Madrigal.html#my-weekend-wrestling-match-with-madrigal-a-mlops-deployment-story",
    "title": "My Weekend Wrestling Match with Madrigal",
    "section": "My Weekend Wrestling Match with Madrigal: A MLOps Deployment Story",
    "text": "My Weekend Wrestling Match with Madrigal: A MLOps Deployment Story\nYou know that feeling? You see a cool project online – in my case, Madrigal, a Red Teaming MLOps platform – glance at the architecture diagram, find the code, and think, “Hey, I could totally run this locally!” My machine’s got the chops, so why not dive in over the weekend? Famous last words, right? What followed was a bit of a rollercoaster ride through the highs and lows of setting things up, hitting walls, and having those little “aha!” moments. Grab a cuppa, and let me tell you about it.\nDay 1: The Multipass Mystery\nFired up and ready to go, I started with Multipass. It’s Canonical’s tool for spinning up Ubuntu VMs pretty easily, and I paired it with MicroK8s for a lightweight Kubernetes setup. The plan? Get a local dev environment rocking for some ML workflow deployment.\nI spent a good chunk of time wrestling with Helm charts and getting the Kubernetes cluster humming. But then, roadblock #1: I set up port-forwarding, but my services were playing hide-and-seek – completely inaccessible from outside the VM. After digging around, it seemed like Multipass’s networking, while great for simple stuff, gets a bit tricky when you need the more advanced routing Kubernetes often demands. Maybe I just didn’t grok the docs well enough, but it felt like hitting a wall for this kind of workload. I eventually figured out that forwarding port to 0.0.0.0:port would work instead of localhost, but it’s too complex for my taste.\n\nThe realization: Picking the right virtualization tool is key. For complex K8s setups, you really need something with beefy, flexible networking options.\n\nDay 2: ARM Wrestling with Kubeflow\nOkay, time for Plan B. I pivoted to k3d, which wraps k3s (a lightweight K8s) in Docker containers. Seemed slick and promising! That is, until I tried deploying Kubeflow Pipelines (KFP). Pods started crashing left and right (CrashLoopBackOff, my old nemesis). A bit of detective work (kubectl logs...) revealed the culprit: the container images for kfp weren’t built for my Mac’s ARM64 architecture.\nSeriously? With Apple Silicon Macs being so common now, I was surprised this was still a snag. Turns out, it’s a known thing, and getting official Kubeflow images to play nice on ARM isn’t always straightforward. Some folks have even created their own modified manifests using alternative ARM64-compatible images just to get things working github.com. It seems deploying KFP locally on a Mac might still be a bit niche. Others trying this on Apple Silicon have found tools like Rancher Desktop helpful for managing Kubernetes locally fmind.medium.com, which might be worth exploring next time.\n\nThe facepalm moment: Always check if your container images support your chip architecture (like ARM64)! It saves a lot of debugging headaches.\n\nDay 3: Phoning a Friend (aka the Creator)\nHitting more snags than I anticipated, I decided to do something radical: I reached out to Aishwarya, Madrigal’s creator. Turns out, he’d deployed it smoothly on a good ol’ Linux VM in DigitalOcean. Sometimes(or oftentimes) the simplest path is the best! So, I decided to follow suit and spin up a cloud VM. Lo and behold, things went much smoother. Kubeflow Pipelines and the Ray cluster popped up, ready for action.\n\nThe reminder: Don’t be afraid to ask! Creators are usually happy to help, and sometimes a quick chat can save hours of frustration. They’re human too!\n\nDay 4: Helm Chart Headaches on the Cloud\nFeeling confident with the cloud setup, I thought, “Let’s level up! How about a managed Kubernetes cluster on DigitalOcean?” Got the cluster running, tried deploying KFP again using Helm charts. Pods were green this time! Success? Not quite. The pipelines wouldn’t actually run.\nThis time, the gremlins were hiding in the Helm chart configurations. Service accounts were missing, security policies weren’t quite right for my setup (especially around the cache-deployer component and MinIO’s security settings). I also realized the charts I pulled were likely bleeding-edge (main/head), so I dialed back to a stable version tag like those mentioned in the official docs kubeflow.org and applied the necessary tweaks.\n\nThe Helm Chart Hindsight: Helm is awesome, but don’t treat charts as magic black boxes. Peek inside, understand the defaults (especially security settings!), and be prepared to customize them for your specific cluster environment.\n\nDay 5: MLflow’s Postgres Password Puzzle\nNext up: integrating MLflow for tracking experiments. Deployed it, and… authentication errors connecting to its PostgreSQL database. Grrr. I triple-checked the passwords I’d set in the Helm values, they looked right.\nThe issue? A classic config mismatch. The credentials defined in the Helm chart values weren’t perfectly matching what ended up in the Kubernetes secrets that the MLflow pods were actually using. Deployment, after re-deployments and some more re-deployment in ArgoCD tends to generate this issue. Pro-tip I learned: sometimes ArgoCD’s “Hard Refresh” can help shake loose weird caching issues when debugging Secret updates.\n\nThe Configuration Consistency Check: Keep your config files and your Kubernetes secrets perfectly in sync. It sounds obvious, but it’s a common tripwire! Hit that hard Refresh button!!\n\nBonus Round: When Libraries Don’t Play Nice\nJust when I thought I was cruising, a Python error popped up while using Hugging Face’s Transformers library: a weird ValueError mentioning sph_legendre_p from SciPy. This usually screams “version conflict!” Sure enough, the versions of SciPy and NumPy installed in my environment weren’t compatible cousins.\n\nThe Dependency Dance: Python environments, especially in ML, are delicate ecosystems. Keep dependencies compatible and managed carefully (using tools like pip freeze &gt; requirements.txt or Poetry/Conda environments).\n\nThe Final Boss: Not Enough Juice!\nAfter squashing all those bugs, I finally had everything deployed. I submitted pipeline jobs, Ray cluster tasks… and nothing happened. The jobs were accepted, but they just sat there, refusing to actually execute. The logs weren’t screaming errors anymore. What gives?\nTurns out, the final hurdle was the most basic: my Kubernetes cluster (the managed one) was just too small! It didn’t have enough CPU or memory resources to actually run the demanding Kubeflow and Ray workloads I was throwing at it.\n\nThe Resource Reality: Size matters! Make sure your Kubernetes cluster has enough CPU, memory, and potentially GPU resources for the workloads you plan to run. I had to scale my cluster to allow it to scale upto 9 nodes, though it only scaled till 6. Under-provisioning leads to silent failures or jobs stuck in pending limbo. Some deployment issues on platforms like k3d can sometimes be resource-related, even if it’s not immediately obvious like running out of memory.\n\nWrapping Up the Weekend\nPhew! What a weekend. It was a whirlwind of debugging, learning, and tweaking. From local setup snags to cloud configuration complexities, every problem solved felt like a mini-victory and taught me something valuable about building out an MLOps platform.\nMy main takeaways?\n\nCheck Tool Fit: Make sure your tools (VMs, K8s distros, container images) actually work with your hardware (hello, ARM64!) and networking needs.\nKnow Your Network: Simple VMs might not cut it for complex K8s networking.\nHelm Isn’t Magic: Understand and tweak Helm charts, especially security bits.\nConfig Consistency: Keep Helm values and K8s secrets aligned.\nResource Right-Sizing: Give your K8s cluster enough power to run your stuff!\n\nHuge thanks to the communities and developers behind Multipass, MicroK8s, k3d, Kubeflow, Ray, ArgoCD, MinIO, PostgreSQL, MLflow, and Transformers. Couldn’t have even attempted this without their work!\nIf you’ve wrestled with similar MLOps deployment dragons or have tips to share, drop a comment! Always keen to learn from others navigating this fast-moving space. Let’s keep building cool things!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Piyush’s Musings on Practical AI",
    "section": "",
    "text": "Mysterious Databricks Bundle, DBT, GitHub Actions, and a Misleading Error\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMy RAG Misadventure: Guardrails, Open Routers, Ragas\n\n\n\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMy Weekend Wrestling Match with Madrigal\n\n\n\n\n\nApr 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Inertia Model and Its Relevance to AI Models\n\n\n\n\n\nApr 3, 2025\n\n\nPiyushkumar Jain\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Convolutional Kernel Transform\n\n\n\n\n\nJul 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to CVAT\n\n\n\n\n\nJul 11, 2020\n\n\nPiyushkumar Jain\n\n\n\n\n\n\n\n\n\n\n\n\nDeployment Journey of a Reinforcement Learning Algorithm\n\n\n\n\n\nJul 7, 2020\n\n\nPiyushkumar Jain\n\n\n\n\n\nNo matching items"
  }
]