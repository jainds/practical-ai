[
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html",
    "title": "All you need to know about CVAT",
    "section": "",
    "text": "Once you start your journey into data science, you quickly learn that as part of your job you are spending more time with data than models. Data and labels go hand in hand and hence, I will be sharing what you need to know before you decide to use CVAT as your image/video annotation tool. Let’s start with the easier bit i.e. complete name of tool - Computer Vision Annotation Tool"
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#introduction",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#introduction",
    "title": "All you need to know about CVAT",
    "section": "",
    "text": "Once you start your journey into data science, you quickly learn that as part of your job you are spending more time with data than models. Data and labels go hand in hand and hence, I will be sharing what you need to know before you decide to use CVAT as your image/video annotation tool. Let’s start with the easier bit i.e. complete name of tool - Computer Vision Annotation Tool"
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-setup",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-setup",
    "title": "All you need to know about CVAT",
    "section": "Ease of Setup",
    "text": "Ease of Setup\nA treat awaits in this section if you have worked with docker compose before, CVAT is very easy to setup as the source code contains necessary docker compose files which makes complete setup a breeze in your local. Repo also features an instruction documentation which is comprehensively written and covers every line of code that needs to be executed in order to get the tool running.\nComplete setup is nicely segregated into docker-compose files with docker apps named cvat,cvat_ui, cvat_db. There are two ways to go ahead with setting up the tool, the easier way of deploying it in a VM and a little longer way on Kubernetes. A VM with enough firepower to run an instance of Postgres, Django backend and a react app to support the number of users that you expect to access the tool simultaneously should be alright. An example: I was able to support 200+ users(some of them were automated scripts) in a single VM with the analytics component running using a Standard D5 v2 (16 vcpus, 56 GiB memory) Azure VM. The power of your VM needs to be proportionately increased if you wish to use the additional components like deep learning model based auto labelling. If you choose to deploy the tool in Kubernetes, you could benefit from the auto-scaling functionality for each app inside CVAT. Kubernetes YAML files are not available as part of source code and hence you might need to create them yourself. I recommend Kompose to create Kubrnetes YAML from docker compose files. Scalability is an important aspect while considering a tool for production deployment. In order to make sure that the tool works for foreseeable growth in number of users. App can be scaled easily since everything is nicely wrapped into docker containers."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-usage",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#ease-of-usage",
    "title": "All you need to know about CVAT",
    "section": "Ease of Usage",
    "text": "Ease of Usage\nThe application’s desktop UI although not fancy, is very feature rich and achieves the goal of labeling images and videos with ease.There’s a long list of keyboard shortcuts supported and you don’t necessarily need to remember every shortcut, simply pick and choose which help you speed up. I found the shortcut to create poly shapes and rectangles during labelling as very useful. Keyboard shortcuts combined with the feature rich app make labelling task slick and smooth. The application also features a task assignment and task process flow using which larger teams can collaborate by assigning tasks to a specific user and updating the current status of task for others to see. CVAT is developed considering the desktop based user interface, which means we need to keep expectations lower while trying to use it on mobile or tablets. You can try the app online right now by navigating to cvat.org CVAT also features a command line interface which enables you to perform simple CRUD operations on task."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#data-extraction-upload",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#data-extraction-upload",
    "title": "All you need to know about CVAT",
    "section": "Data Extraction/ Upload",
    "text": "Data Extraction/ Upload\nA Django app in backend and react UI as frontend, CVAT is quite covered with options to upload data via UI or CLI. App works on a unique task based system, each upload is created as a task in the system. The task can then be further assigned to different users for labelling, quality check, etc. When using the UI to upload data, you don’t need to worry about label formatting because the app takes care of it. CLI based upload requires data to be structured in specific formats before it can be processed. I recommend CLI to perform automated scripts based data upload and UI when actual human is performing the uplaod and labelling task.\nExtraction of data is a very important step which will have to be performed by every team on regular basis. CVAT supports extraction of data in a format by both interfaces i.e. UI and CLI. A user can go to a task and there’s option to extract the data in various supported formats. when trying to extract multiple data points or tasks in CVAT, UI based extraction might seem time consuming . CLI comes to rescue here, extraction of data in any supported format is super simple using CLI. An important thing to note here is CVAT currently doesn’t support bulk extraction or upload of data using UI.\nThe dev team also mentiond about datumaro dataset framework which can be used to transform, merge, extract multiple datasets from CVAT. I was not able to get it working and therefore no comments on that."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#annotations-format-supported",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#annotations-format-supported",
    "title": "All you need to know about CVAT",
    "section": "Annotations Format Supported",
    "text": "Annotations Format Supported\nI am borrowing a table available in CVAT documentation to show the formats supported. It supports all major community defined data label formats. The labels covers the spectrum of classification, obejct detection and segmentation tasks in computer vision.\n\n\n\nAnnotation format\nImport\nExport\n\n\n\n\nCVAT for images\nX\nX\n\n\nCVAT for a video\nX\nX\n\n\nDatumaro\n\nX\n\n\nPASCAL VOC\nX\nX\n\n\nSegmentation masks from PASCAL VOC\nX\nX\n\n\nYOLO\nX\nX\n\n\nMS COCO Object Detection\nX\nX\n\n\nTFrecord\nX\nX\n\n\nMOT\nX\nX\n\n\nLabelMe 3.0\nX\nX"
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#backup-and-restore",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#backup-and-restore",
    "title": "All you need to know about CVAT",
    "section": "Backup and Restore",
    "text": "Backup and Restore\nAll images/videos uploaded are stored in docker volume cvat_data and the respective label data is stored in postgres. Postgres data is stored in docker volume cvat_db. In order to backup the complete app data, you can simply create volume backup for these two volumes in form of .tar files. Configuring the said docker volumes to a persistent storage like S3 or azure blob would enable you to setup automated cloud backup for these volumes. Restoration is as simple as backup by using docker commands."
  },
  {
    "objectID": "posts/2020-07-11-CVAT-All-you-need-to-know.html#community",
    "href": "posts/2020-07-11-CVAT-All-you-need-to-know.html#community",
    "title": "All you need to know about CVAT",
    "section": "Community",
    "text": "Community\nCVAT community is available on GitHub and Gitter. I have personally found them responding faster on gitter compared to raising issues on github."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/inertia/inertia.html",
    "href": "posts/inertia/inertia.html",
    "title": "The Inertia Model and Its Relevance to AI Models",
    "section": "",
    "text": "The Inertia Model, as described in The Great Mental Models series, explains how objects—or systems—tend to resist change due to existing momentum or stagnation. In physics, inertia keeps a moving object in motion unless acted upon by an external force. The same principle applies to decision-making, businesses, and even artificial intelligence models.\nAI models, once trained, exhibit a form of inertia. They rely on the data they were trained on and the assumptions built into their architectures. Without intentional updates or external intervention, they continue making predictions based on historical patterns, even when those patterns become outdated. For example, a recommendation algorithm trained on past user behavior may struggle to adapt to sudden shifts in preferences unless it is retrained with fresh data. Another example of inertia, traditional businesses like banking and insurance continue to struggle in adopting latest AI technologies in their core areas.\nAnother aspect of inertia in AI is model bias. If an AI system has learned biased associations from historical data, that bias will persist unless corrective measures—such as better data curation or active bias mitigation—are introduced. The resistance to change mirrors human cognitive inertia, where existing beliefs and habits are difficult to alter without strong external stimuli.\nApplications and Decision making, especially in relation to AI, chimes well with this model. AI models/applications that are incrementally increasing/improving certain parts of a business process are leveraging existing momentum. Comparatively easier to explain the significance, value and cost to the business. It aligns well with the direction and movement of the organization and finds easier/wider acceptance in the organisation. Consider an AI model/application that is fundamentally disrupting the existing process or probably making it altogether redundant; they require significantly more escape velocity (so to speak, in cost, explanation, value) to achieve the desired results and hardly see the light of the day.\nIn a broader sense, recognizing inertia in AI systems helps researchers and practitioners understand the importance of continuous learning and adaptation. Just as overcoming inertia in human behavior requires conscious effort, ensuring AI models remain effective and relevant requires ongoing intervention, retraining, and validation.\nBy applying the Inertia Model, we can better anticipate challenges in AI development and deployment, ensuring that models do not stagnate or reinforce outdated patterns, but instead evolve with the real-world data they interact with."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Piyush’s Musings on Practical AI",
    "section": "",
    "text": "“Deployment Journey of a Reinforcement Learning Algorithm”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Random Convolutional Kernel Transform”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll you need to know about CVAT\n\n\n\n\n\n\nannotation\n\n\n\nGet to know Computer vision annotation tool.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Inertia Model and Its Relevance to AI Models\n\n\n\n\n\nPost description\n\n\n\n\n\nApr 3, 2025\n\n\nPiyushkumar Jain\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 4, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 1, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nHi there, Welcome to Piyush.Dev\nI am a data scientist who enjoys playing with new technologies and identifying possible use cases in the real world.\nI am an engineer by qualification, turned to data science by sheer nature of curiosity.\nI like learning new things and sharing here.\nWelcome to my site.\nThis website is now powered by Quarto"
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html",
    "href": "posts/2020-07-07-Deploying-RL-Model.html",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "",
    "text": "“Step by step code based deploymeny into production AKS serving millions of simultaneous users”"
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#introduction",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#introduction",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Introduction",
    "text": "Introduction\nOur team is working to improve the health and wealth of millions of current customers and acquire more customers in the future. One of the most effective and efficient way to achieve our goal is by getting an app into the millions of people. As it turns out, we already have a wonderful application which is downloaded by more than 3 million users as I write this post. The mobile application has a carousel portion in the bottom half section of the home page where dynamic banners can be rendered. Each banner is utilized as a form of information, communication medium or an application feature. This is the first page that is seen by all users who successfully register and a portion of them clicking on the banner displayed registering their interest. Our team’s goal is to increase engagement within the app. The first step was to understand the source of users who were clicking the banners, why are they willing to go into exploring app via banners after registering while others would go on to explore the app via other routes."
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#problem-statement",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#problem-statement",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Problem statement",
    "text": "Problem statement\nThe goal was to increase user engagement within the app by understanding user’s interest in a variety of banners and then leverage the results across the app.\nwe didn’t have existing data about user interaction with the app neither did we have enough time at hand to perform that activity. We were also looking at an incoming huge inflow of new users expected in near future due to the planned marketing campaigns. We were essentially looking at a cold start problem to improve engagement since, we would know little about the new users and time to market was a very important factor. We were expected to go live within two weeks duration with a solution to make the best out of data available at hand."
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#solution",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#solution",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Solution",
    "text": "Solution\nBayesian bandits with Thomson sampling ticked all boxes as follows: 1. It requires no data or less to start with compared to other options 2. It will learn incoming users/data and start recommending banners 3. Can work with new banners configured as new arms\nThe next phase of the project was also discussed where we agreed to work on building contextual bandits. In this post, I will be talking more about how we used various tools and technology making deployment possible. I will not be talking about how the recommendation algorithm works and the technology stack used to achieve it."
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#deployment",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#deployment",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Deployment",
    "text": "Deployment\n\nThe build and deployment part of the project was broken down into two technical stages/phases: 1. Testing model and documenting results in the pre-prod environment with production data, define the input and the output schema for the model which will be used by the data engineer team to create a streaming pipeline. 2. Setup model to consume a live stream of event data, and respond via a REST endpoint with the recommended list of banners for the users\nThe front end of the mobile app is configured for a response time of one second w.r.t to back-end. It meant that the app will try to generate dynamic banners on the user screen based on our recommendations or fall back to static banners if we failed to deliver a response within a second, which added another layer of complexity to the second stage. Our APIs were expected to support a wide range of user load starting from a few hundred requests to millions across the region.\nWe could list the deployment infra into three major components: 1. A robust build and deployment pipeline 2. Automated performance testing 3. Production monitoring and alerting"
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#tools",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#tools",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Tools",
    "text": "Tools\nTools used for the complete setup: 1. Jenkins 2. Artifactory 3. Docker 4. Aquasec image scanning 5. Fortify static code scan 6.. Sonar Nexus open source code scanning 7. Kubernetes 8. Predator 9. Prometheus 10. Grafana 11. Bitbucket\nOur application solution is a bunch of docker images which consumes/produces content in Kafka topics.\n\nStep 1 — Fetching code and checking for changes\nOur pipeline starts at fetching the code from Bitbucket repository. We store code in the folder structure for the 4 different docker images that are to be built. We check whether a file has been changed before initiating build for the files in that folder. The code in the Jenkins pipeline is as below for one of the folders titled ‘generator’.\n\n#hide_output\nscript{\n            GIT_RESULT = sh(script: '''git diff --quiet HEAD \"$(git rev-parse @~1)\" -- generator''',\n                returnStatus: true\n                )\n                echo \"GIT RESULT -- ${GIT_RESULT} -- ${params.branchname}\"\n              }\n\n\n  File \"&lt;ipython-input-3-822414454e3f&gt;\", line 2\n    script{\n          ^\nSyntaxError: invalid syntax\n\n\n\n\n\n\nStep 2 - Fortify\nNext step is to run complete code static security scanning by Fortify\n\n#hide_output\nsh '''\n      echo \"==================================================\"\n      echo \"========--- SAST - Fortify Scan: Start ---========\"\n      echo \"==================================================\"\n      hostname\n      whoami\n      ls -ahl\n      echo 'WORKSPACE: ' $WORKSPACE\n      cd $WORKSPACE\n      pwd\n      sourceanalyzer -v\n      sourceanalyzer -b ${fortify_app_name} -clean\n      sourceanalyzer -b ${fortify_app_name} -python-version ${python_version} -python-path ${python_path} ${fortify_scan_files}\n      sourceanalyzer -b ${fortify_app_name} -scan -f ${fortify_app_name}.fpr\n      fortifyclient -url https://sast.intranet.asia/ssc -authtoken \"${fortify_upload_token}\" uploadFPR -file ${fortify_app_name}.fpr -project ${fortify_app_name} -version ${fortify_app_version}\n     '''\n\n\n\nStep 3 - Docker\nThe next step is to build the docker image. We first login to Artifactory before initiating the build as our pip libraries are also pulled from mirrored pip in the Artifactory. I have provided a sample of code on how we achieve this.\n\n#hide_output\nsh \"\"\"\n                echo ${ARTIFACTORY_PASSWORD} | docker login -u ${ARTIFACTORY_USERNAME} --password-stdin docker-registry:8443\n                cd generator\n                docker build --file Docker-dev \\\n                 --build-arg HTTPS_PROXY=http://ip-address \\\n                 --build-arg ARTIFACTORY_USERNAME=${ARTIFACTORY_USERNAME} \\\n                 --build-arg ARTIFACTORY_PASSWORD=${ARTIFACTORY_PASSWORD} \\\n                 -t ${env.generator_image_latest} .\n                docker tag ${env.generator_image_latest} ${env.generator_image_name}\n                docker push ${env.generator_image_latest}\n                docker push ${env.generator_image_name}\n                docker logout docker-pcaaicoe.pruregistry.intranet.asia:8443\n                cd ..\n    \"\"\"\n\n\n  File \"&lt;ipython-input-4-b4a310bd01e3&gt;\", line 15\n    \"\"\"\n       \n^\nSyntaxError: invalid syntax\n\n\n\n\n\n\nStep 4 - Aquasec\nAfter pushing an image into Artifactory, the next important and mandatory step to be performed is docker image security scanning.\n\n#hide_output\n    sh \"\"\"\n         echo \"==================================================\"\n         echo \"=============--- OSS - Nexus Scan ---=============\"\n         echo \"==================================================\"\n                docker save -o generator-dev.tar ${env.generator_image_latest}\n                \"\"\"\n                String result = nexusscan(\"pcaaicoeaipulsenudgesgeneratordev\", \"$WORKSPACE\", \"build\");\n                echo result;\n                sh \"\"\"\n                rm -f generator-dev.tar\n                \"\"\"\n                sh \"\"\"\n                echo \"==================================================\"\n                echo \"=============--- CSEC - Aquasec Scan ---==========\"\n                echo \"==================================================\"\n    \"\"\"\n                aquasecscan(\"${env.generator_image_latest}\")\n\n\n  File \"&lt;ipython-input-5-c8af31814524&gt;\", line 7\n    \"\"\"\n       \n^\nSyntaxError: invalid syntax\n\n\n\n\nThe code and image security scanning stages are major milestones to be cleared during the deployment phase. It is important as well as difficult to explain and agree between application security teams about what risks are we willing to take while allowing open source libraries with bugs to go live in our environment.\n\n\nStep 5 — Kubernetes\nNow we move on to the stage where we will be able to actually deploy and run our images. In order to deploy our solution, we need a Redis DB and Kafka cluster up and running. We deploy our docker images using the below code:\n\n#hide_output\nsh '''\n            set +x\n            echo \"---- preparing options ----\"\n            export HTTPS_PROXY=ip-address:8080\n            export KUBE_NAMESPACE=\"internal-namespace\"\n            export KC_OPTS=${KC_OPTS}\" --kubeconfig=${KUBE_CONFIG}\"\n            export KC_OPTS=${KC_OPTS}\" --insecure-skip-tls-verify=true\"\n            export KC_OPTS=${KC_OPTS}\" --namespace=${KUBE_NAMESPACE}\"\n            \n            echo \"---- prepared options ----\"\n            echo \"---- preparing alias ----\"\n            alias kc=\"kubectl ${KC_OPTS} $*\"\n            echo \"---- alias prepared ----\"\n            \n            echo \"---- applying manifest ----\"\n   \n   \n           kc apply -f configmap.yaml\n\n           if [ $which_app = \"generator\" ];then\n             if [ $image_version = \"latest\" ];then\n               kc delete deploy ai-pulse-nudges-events-reader||echo\n             fi\n             sed -i \"s!GENERATOR_VERSION!$image_version!g\" \"generator.yaml\"\n             kc apply -f generator.yaml\n           fi  \n\n           if [ $which_app = \"aggregator\" ];then\n             if [ $image_version = \"latest\" ];then\n               kc delete deploy ai-pulse-nudges-click-counter||echo\n             fi\n             sed -i \"s!AGGREGATOR_VERSION!$image_version!g\" \"aggregator.yaml\"\n             kc apply -f aggregator.yaml\n           fi\n\n           if [ $which_app = \"detector\" ];then\n             if [ $image_version = \"latest\" ];then\n               kc delete deploy ai-pulse-nudges-engine||echo\n             fi\n             sed -i \"s!DETECTOR_VERSION!$image_version!g\" \"detector.yaml\" \n             kc apply -f detector.yaml\n           fi\n\n           if [ $which_app = \"restapi\" ];then\n             if [ $image_version = \"latest\" ];then\n               kc delete deploy ai-pulse-nudges-restapi||echo\n                fi\n             sed -i \"s!REST_VERSION!$image_version!g\" \"restapi.yaml\"\n             kc apply -f restapi.yaml\n                    fi\n            if [ $which_app = \"all\" ];then\n             if [ $image_version = \"latest\" ];then\n             kc delete deploy ai-pulse-nudges-events-reader||echo\n             kc delete deploy ai-pulse-nudges-click-counter||echo\n             kc delete deploy ai-pulse-nudges-engine||echo\n             kc delete deploy ai-pulse-nudges-restapi||echo\n             fi\n\n             sed -i \"s!GENERATOR_VERSION!$image_version!g\" \"generator.yaml\"\n             sed -i \"s!AGGREGATOR_VERSION!$image_version!g\" \"aggregator.yaml\"\n             sed -i \"s!DETECTOR_VERSION!$image_version!g\" \"detector.yaml\" \n             sed -i \"s!REST_VERSION!$image_version!g\" \"restapi.yaml\"\n\n             kc apply -f generator.yaml\n             kc apply -f aggregator.yaml\n             kc apply -f detector.yaml\n             kc apply -f restapi.yaml\n                    fi\n   \n   \n   \n            echo \"---- manifest applied ----\"\n            echo \"---- checking result ----\"\n            \n            echo \" &gt;&gt; Deployments \"\n            kc get deployments\n            \n            echo \" &gt;&gt; Services\"\n            kc get svc\n            \n            echo \" &gt;&gt; Ingress\"\n            kc get ingress\n            \n            echo \" &gt;&gt; Pods\"\n            kc get pods\n            \n            echo \"---- Done ----\"\n          '''\n\n\n  File \"&lt;ipython-input-6-5e40e7794776&gt;\", line 89\n    '''\n       \n^\nSyntaxError: invalid syntax\n\n\n\n\n\n\nStep 6 — Performance test\nWe deploy Predator — the tool which we use for performance test.\n\n#hide_output\nsh '''\n            set +x\n            echo \"---- preparing options ----\"\n            export HTTPS_PROXY=ip-address:8080\n            export KUBE_NAMESPACE=\"internal-namespace\"\n            export KC_OPTS=${KC_OPTS}\" --kubeconfig=${KUBE_CONFIG}\"\n            export KC_OPTS=${KC_OPTS}\" --insecure-skip-tls-verify=true\"\n            export KC_OPTS=${KC_OPTS}\" --namespace=${KUBE_NAMESPACE}\"\n            \n            echo \"---- prepared options ----\"\n            echo \"---- preparing alias ----\"\n            alias kc=\"kubectl ${KC_OPTS} $*\"\n            echo \"---- alias prepared ----\"\n            \n            echo \"---- applying manifest ----\"\n   \n           kc get deploy|grep predator|awk '{print $1 }' || echo\n           kc get deploy|grep predator|awk '{print $1 }'|xargs kc delete deploy || echo\n\n           for i in `seq $replica_count`\n           do\n             echo $i\n             cp -rf predator/predator.yaml tmp.yaml\n             sed -i \"s!REPLICA_NO!$i\"\"!g\" \"tmp.yaml\"\n             kc apply -f tmp.yaml\n           done  \n   \n   \n            # kc apply -f predator/predator.yaml\n            \n            echo \"---- manifest applied ----\"\n            echo \"---- checking result ----\"\n            \n            echo \" &gt;&gt; Deployments \"\n            kc get deployments\n            \n            echo \" &gt;&gt; Services\"\n            kc get svc\n            \n            echo \" &gt;&gt; Ingress\"\n            kc get ingress\n            \n            echo \" &gt;&gt; Pods\"\n            kc get pods\n            \n            echo \"---- Done ----\"\n          '''\n\n\n  File \"&lt;ipython-input-7-c633d3acb00f&gt;\", line 48\n    '''\n       \n^\nSyntaxError: invalid syntax\n\n\n\n\nPredator is an amazing tool that enables us to leverage existing Kubernetes infra for an unlimited number of users for testing. Read more about the tool here: https://medium.com/zooz-engineering/by-niv-lipetz-software-engineer-zooz-b5928da0b7a8 We leverage the existing enterprise Prometheus and Grafana set up to monitor the application pods."
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#lessons-learned-for-next-time",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#lessons-learned-for-next-time",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Lessons learned for next time:",
    "text": "Lessons learned for next time:\n\nWe started writing the pipeline code from scratch, whereas it would have helped save time if an advanced hello world type of empty pipeline existed, which could be used as a template structure. It would have enabled us to know what credentials and access were required at what stage.\nThere were many credentials and access that were required to get the pipeline up and running. It would be a time and effort savior if we have one master service id created and assigned to a pipeline which can then be used across all tools in the organization.\nIt is very difficult to build a machine learning model, and real-time streaming data was an additional complexity, but productionizing that model with streaming data is many folds difficult."
  },
  {
    "objectID": "posts/2020-07-07-Deploying-RL-Model.html#contributors",
    "href": "posts/2020-07-07-Deploying-RL-Model.html#contributors",
    "title": "“Deployment Journey of a Reinforcement Learning Algorithm”",
    "section": "Contributors",
    "text": "Contributors\nGlenn Bayne, Tien Nguyet Long, John Yue, Zeldon Tay （郑育忠), Steven Chau , Denys Pang , Philipp Gschoepf , Syam Bandi , Uma Maheshwari, Michael Natusch"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "",
    "text": "“Understanding the ROCKET paper step by step with an example comparing results with LSTM”"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#introduction",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#introduction",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Introduction",
    "text": "Introduction\nROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels is a research paper published in October 2019 by Angus Dempster, François Petitjean, Geoffrey I. Webb. The paper presents a unique methodology to transform time series data using convolutional kernels to improve classification accuracy. This paper is unique in learning from the recent success of convolutional neural networks and transferring it on time-series datasets.\nThe link to download the paper from arxiv - Paper"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#time-series-data",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#time-series-data",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Time Series data",
    "text": "Time Series data\nTime series data is defined as a set of data points containing details about different points in time. Generally, time-series data contains data points sampled/observed at an equal interval of time. Time series classification is task of identifying patterns and signals in the data in relation to respective classes.\nThe proposal is features generated by the convolution of randomly generated kernels on time series data results in faster and better time series classifiers. We will go into more detail of this proposal and understand how the methodology proposed by them helps to improve the accuracy."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#kernels",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#kernels",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Kernels",
    "text": "Kernels\nKernels in simple terms is a small matrix used to modify the images. Let’s try to understand kernels using an example:\nhere is a 3 x 3 kernel used to sharpen images:\n\\(\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0  \\end{bmatrix}\\)\nTo sharpen an image using the above kernel, we need to perform a dot product of each pixel in the image with the kernel matrix. The resulting image would then be a sharpened version of the original image. Observe the gif below to see a live version of the kernel dot product in motion.\nFollowing is an example from setosa.io site to demonstrate how kernels can be used to make desirable changes to an image.\n\n\n5 parameters of kernels\nA kernel has 5 different parameter using which it can be configured.\n\n\n\n\n\n\n\n\nParameter\nDescription\nValue logic\n\n\n\n\nBias\nBias is added to the result of the convolution operation between input time series and weights of the given kernel\nBias is sampled from a uniform distribution, b ∼ U(−1,1)\n\n\nSize(Length)\nSize defines the number of rows and columns a kernel has. The above example has a size of 3 rows and 3 columns\nLength is selected randomly from {7,9,11} with equal probability, making kernels considerably shorter than input time series in most cases\n\n\nWeights\nThe values that make up the kernel matrix are weights\nThe weights are sampled from a normal distribution, ∀w ∈ W, w ∼ N(0,1), and are mean-centered after being set, ω = W − W. As such, most weights are relatively small, but can take on larger magnitudes\n\n\nDilation\nDilation spreads a kernel over the input such that with dilation of value two, weights in a kernel are convolved with every second element of input time series\nDilation is sampled on an exponential scale d = ⌊2x⌋,x ∼ U(0,A), input −1 where A = log2 kernel −1\n\n\nPadding\nPadding involves appending values(typically zero) to the start and end of input time series such that the middleweight of a kernel aligns with the first value of input time series at the start of convolution\nWhen each kernel is generated, a decision is made (at random, with equal probability) whether or not padding will be used when applying the kernel"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#features-generated-by-rocket-kernel",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#features-generated-by-rocket-kernel",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Features generated by Rocket kernel",
    "text": "Features generated by Rocket kernel\nRocket computes two aggregate features from each kernel and feature convolution. The two features are created using the well-known methodology global/average max pooling and a unique methodology positive proportion value (ppv).\n\nMax-pooling\nGlobal max-pooling is essentially picking the maximum value from the result of convolution and max pooling is picking the maximum value within a pool size. Assuming that the output of convolution is 0,1,2,2,5,1,2, global max pooling outputs 5, whereas ordinary max pooling with pool size equals to 3 outputs 2,2,5,5,5\n\n\nProportion of positive values\nLet’s try to understand using the author’s own words to describe ppv.\n\nppv directly captures the proportion of the input which matches a given pattern, i.e., for which the output of the convolution operation is positive. The ppv works in conjunction with the bias term. The bias term acts as a kind of ‘threshold’ for ppv. A positive bias value means that ppv captures the proportion of the input reflecting even ‘weak’ matches between the input and a given pattern, while a negative bias value means that ppv only captures the proportion of the input reflecting ‘strong’ matches between the input and the given pattern."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-usage",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-usage",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Rocket usage",
    "text": "Rocket usage\nNow that we understand what kernels are and how rocket generates two outputs by convolution of kernel and input vector, let’s understand how to use it.\nThe time-series data needs to be provided as input into the rocket transform method, the value for the number of kernels (i.e. k) is set at 10,000 by default. This means that for each one of the input features we get 20,000 features as output from rocket transform.\nThe transformed feature table can now we used as input data for any classification algorithm, authors advise linear algorithms like ridge regression classifier or logistic regression."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-vs-others",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-vs-others",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Rocket v/s others",
    "text": "Rocket v/s others\nRocket’s approach of creating a large number of random kernels and generating two features from each kernel is unique. Rocket distinguishes itself based on various other factors which we will discuss below.\n\nRocket v/s neural nets\n\nRocket doesn’t use a hidden layer or any non-linearities\nFeatures produced by Rocket are independent of each other\nRocket works with any kind of classifier\n\n\n\nRocket v/s CNN\n\nRocket uses a very large number of kernels\nIn CNN, a group of kernels tend to share the same size, dilation and padding. Rocket has all 5 parameters randomized.\nIn CNN, Dilation increases exponentially with depth; Rocket has random dilation values\nCNNs only have average/max pooling. Rocket has a unique pooling called as ppv which has proven to provide much better classification accuracy on time series."
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-performance",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#rocket-performance",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Rocket performance",
    "text": "Rocket performance\nThe authors provide detailed information about the classification accuracy and time taken to train the model. I am discussing the results from bakeoff datasets in this article and you will be able to find results from various additional datasets in the paper.\n\nAccuracy\nRank is calculated by taking a mean value of classification accuracy across all the 85 datasets in bakeoff datasets.\nIt is clear that the model trained using features derived using rocket is faring better compared to other models on average among all the datasets in bake-off datasets. Please note that the dark horizontal line connecting the rank position of two models depict that the results from two models are not statistically insignificant.\n\n\n\nTime taken to train\n\n\n\n\n\n\n\n\nArchitecture\nLargest dataset(ElectricDevices, with 8,926 training examples)\nLongest time series(HandOutlines, with time series of length 2,709)\n\n\n\n\nROCKET\n6 minutes 33 seconds\n4 minutes 18 seconds\n\n\nMrSEQL\n31 minutes\n1 hour 55 minutes\n\n\ncBOSS\n3 hours 6 minutes\n42 minutes\n\n\nProximity Forest\n1 hour 35 minutes\n3 days\n\n\nTS-CHIEF\n2 hours 24 minutes\n4 days\n\n\nInception Time (on GPU)\n7 hours 46 minutes\n8 hours 10 minutes"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#example",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#example",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Example",
    "text": "Example\nIn the below examples, we are going to try and train a Human activity recognizer time series classifier. I am using two nice repo by Guillaume Chevalier showcasing LSTM model on Human activity recognizer with a classification accuracy of 91% and by Thanatchon . Let’s see how much accuracy can be achieved by using rocket transforms.\nWe will be using sktime implementation of rocket in this example\n\nInstall libraries & import Statements\n\n#hide\n! pip install pandas numba scikit-learn sktime matplotlib\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\nRequirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (0.48.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\nRequirement already satisfied: sktime in /usr/local/lib/python3.6/dist-packages (0.4.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\nRequirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\nRequirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba) (49.1.0)\nRequirement already satisfied: llvmlite&lt;0.32.0,&gt;=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba) (0.31.0)\nRequirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from sktime) (0.34.2)\nRequirement already satisfied: statsmodels&gt;=0.11.0 in /usr/local/lib/python3.6/dist-packages (from sktime) (0.11.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.6.1-&gt;pandas) (1.15.0)\nRequirement already satisfied: patsy&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels&gt;=0.11.0-&gt;sktime) (0.5.1)\n\n\n\n#collapse-hide\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport os\nimport zipfile\n#orchestration\nfrom sklearn.pipeline import make_pipeline\n\n#transforms\nfrom sktime.transformers.series_as_features.rocket import Rocket \n\n#plot\nimport matplotlib.pyplot as plt\n\n#metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Classifier\nfrom sklearn.linear_model import RidgeClassifierCV\n\n\n\nDownload dataset and extract\n\n#collapse-hide\n# Download the file\n!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"\n\n--2020-07-24 06:58:35--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 60999314 (58M) [application/x-httpd-php]\nSaving to: ‘UCI HAR Dataset.zip’\n\nUCI HAR Dataset.zip 100%[===================&gt;]  58.17M  35.0MB/s    in 1.7s    \n\n2020-07-24 06:58:37 (35.0 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314]\n\n\n\n\n#collapse-hide\n# Extract\n\nzip_ref = zipfile.ZipFile('UCI HAR Dataset.zip', 'r')\nzip_ref.extractall('./')\nzip_ref.close()\n\n\n#collapse-hide\n# validate if file exists\n!ls ./'UCI HAR Dataset'\n\nactivity_labels.txt  features_info.txt  features.txt  README.txt  test  train\n\n\n\n#collapse-hide\n# Useful Constants\n\n# Those are separate normalised input features for the neural network\nINPUT_SIGNAL_TYPES = [\n    \"body_acc_x_\",\n    \"body_acc_y_\",\n    \"body_acc_z_\",\n    \"body_gyro_x_\",\n    \"body_gyro_y_\",\n    \"body_gyro_z_\",\n    \"total_acc_x_\",\n    \"total_acc_y_\",\n    \"total_acc_z_\"\n]\n\n# Output classes to learn how to classify\nLABELS = [\n    \"WALKING\", \n    \"WALKING_UPSTAIRS\", \n    \"WALKING_DOWNSTAIRS\", \n    \"SITTING\", \n    \"STANDING\", \n    \"LAYING\"\n]\n\n\n\nDefining Train test data\n\n#collapse-show\nDATA_PATH = \"./\"\nTRAIN = \"train/\"\nTEST = \"test/\"\nDATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n\n\n# Load \"X\" (the neural network's training and testing inputs)\n\ndef load_X(X_signals_paths):\n    X_signals = []\n    \n    for signal_type_path in X_signals_paths:\n        file = open(signal_type_path, 'r')\n        # Read dataset from disk, dealing with text files' syntax\n        X_signals.append(\n            [np.array(serie, dtype=np.float32) for serie in [\n                row.replace('  ', ' ').strip().split(' ') for row in file\n            ]]\n        )\n        file.close()\n    \n    return np.transpose(np.array(X_signals), (1, 2, 0))\n\nX_train_signals_paths = [\n    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \n    \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n]\nX_test_signals_paths = [\n    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \n    \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n]\n\nX_train = load_X(X_train_signals_paths)\nX_test = load_X(X_test_signals_paths)\n\n\n# Load \"y\" (the neural network's training and testing outputs)\n\ndef load_y(y_path):\n    file = open(y_path, 'r')\n    # Read dataset from disk, dealing with text file's syntax\n    y_ = np.array(\n        [elem for elem in [\n            row.replace('  ', ' ').strip().split(' ') for row in file\n        ]], \n        dtype=np.int32\n    )\n    file.close()\n    \n    # Substract 1 to each output class for friendly 0-based indexing \n    return y_ - 1\n\ny_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\ny_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n\ny_train = load_y(y_train_path)\ny_test = load_y(y_test_path)\n\n\nprint('OK !')\n\nOK !\n\n\n\n\nPreparing dataset\n\n#collapse-show\ndef preprocess_data(data_array):\n\n    dim_dict = {}\n\n    for i in range(9):\n        name_i = f'dim_{str(i)}'\n        dim_dict[name_i] = []\n\n    for i in range(data_array.shape[0]):\n        for j in range(data_array.shape[1]):\n            name_dim = f'dim_{str(j)}'\n            dim_dict[name_dim].append(\n                pd.Series(data_array[i][j]).astype('float64'))\n            \n    return pd.DataFrame(dim_dict)\n\nprint('OK !')\n\nOK !\n\n\n\n#collapse-show\nX_train = X_train.reshape(-1,9,128)\nX_test = X_test.reshape(-1,9,128)\n\n%time X_train = preprocess_data(X_train)\n%time X_test = preprocess_data(X_test)\n\ny_train = list(y_train.copy().ravel())\ny_test = list(y_test.copy().ravel())\n\ny_train = [str(each) for each in y_train]\ny_test = [str(each) for each in y_test]\n\nCPU times: user 15.6 s, sys: 382 ms, total: 16 s\nWall time: 15.5 s\nCPU times: user 6.62 s, sys: 282 ms, total: 6.9 s\nWall time: 6.47 s\n\n\n\n\nRocket Model\n\nrocket_pipeline = make_pipeline(\n    Rocket(num_kernels = 10000, random_state = 1),\n    RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n)\n\n%time rocket_pipeline.fit(X_train, y_train)\n\nCPU times: user 15min 33s, sys: 8.24 s, total: 15min 41s\nWall time: 8min 4s\n\n\nPipeline(memory=None,\n         steps=[('rocket',\n                 Rocket(normalise=True, num_kernels=10000, random_state=1)),\n                ('ridgeclassifiercv',\n                 RidgeClassifierCV(alphas=array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n       2.15443469e+02, 1.00000000e+03]),\n                                   class_weight=None, cv=None,\n                                   fit_intercept=True, normalize=True,\n                                   scoring=None, store_cv_values=False))],\n         verbose=False)\n\n\n\n\nAccuracy\n\n%time rocket_pipeline.score(X_test, y_test)\n\nCPU times: user 4min 32s, sys: 193 ms, total: 4min 32s\nWall time: 2min 17s\n\n\n0.9355276552426196\n\n\n\n%time y_pred = list(rocket_pipeline.predict(X_test))\n\nCPU times: user 4min 32s, sys: 128 ms, total: 4min 32s\nWall time: 2min 17s\n\n\n\n%time plot_confusion_matrix(rocket_pipeline, X_test, y_test, display_labels = LABELS)\n\nCPU times: user 4min 32s, sys: 247 ms, total: 4min 32s\nWall time: 2min 18s"
  },
  {
    "objectID": "posts/2020-07-24-Random-convolutional-kernel-transform.html#conclusion",
    "href": "posts/2020-07-24-Random-convolutional-kernel-transform.html#conclusion",
    "title": "“Random Convolutional Kernel Transform”",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to improve upon accuracy achieved by LSTM model by a very simple implementation using Rocket. LSTM had scored 91% and using Rocket + Ridge Regression classifier the accuracy jumped to 93%. The performance gain achieved becomes sweeter when you compare the time required to code and train, which was very small compared for rocket in comparison to LSTM in our case. The Rocket methodology is an innovative, simple and fresh technique that attracted my attention to this research paper."
  }
]