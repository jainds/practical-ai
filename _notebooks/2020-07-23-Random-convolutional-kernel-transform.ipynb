{"cells":[{"metadata":{},"cell_type":"markdown","source":"# \"Random Convolutional Kernel Transform\"\n> \"understand Rocket paper in simple words\"\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- categories: [research paper, time series]\n- image: images/deployment-journey-reinforcement-learning.png\n- hide: false\n- search_exclude: false\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n  ***ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels***  is a reasearch paper published in October 2019 by Angus Dempster, François Petitjean, Geoffrey I. Webb. The paper presents a unique methodology to transform time series data using convolutional kernels in order to improve classification accuracy. This paper is unique in learning from recent success of convolutional neural networks and transferring it on time series datasets. \n\nThe link to download the paper from arxiv - [Paper](https://arxiv.org/pdf/1910.13051)\n\n\n\n## Time Series data\n\nTime series data is defined as set of data points containing details about different point in time. Generally time series data contains data points sampled/observed at equal interval of time. Time series classification can be imagined as identifying patterns and signals in time series data in relation to respective classes. \n\nAuthors in this paper are promising fast and accurate time series classification. they propose that generating features using randomly generated kernels on time series data results in much btter accuracy.\n\n## Kernels\n\nKernels in simple terms is a small matrix used to modify the images. Let's try to understand kernels using an example: \n\nhere is a 3 x 3 kernel used to sharpen images: \n\n$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0  \\end{bmatrix}$\n\nIn order to sharpen an image using above kernel, we need to perform a dot product of each pixel in image with the kernel matrix. The resulting image would then be a sharpened version of original image. Observe the gif below to see a live version of kernel dot product in motion. \n\nFollowing is an example from setosa.io site to demonstrate how kernels can change the images. \n\n### 5 parameters of kernels\n\nA kernel has 5 different parameter using which it can be configured. \n\n| Parameter      | Description | Value logic |\n| ----------- | ----------- | --------- |\n| Bias      | Bias is added to the result of the convolution operation between input time series and weights of the given kernel | Bias is sampled from a uniform distribution, b ∼ U(−1,1) |\n| Size(Length)   | Size defines the number of rows and columns a kernel has. The above example has a size of 3 rows and 3 colums | Length is selected randomly from {7,9,11} with equal probability, making kernels considerably shorter than input time series in most cases |\n| Weights | The values that make up the kernel matrix are weights  | The weights are sampled from a normal distribution, ∀w ∈ W, w ∼ N(0,1), and are mean centered after being set, ω = W − W. As such, most weights are relatively small, but can take on larger magnitudes |\n| Dilation | Dilation spreads a kernel over the input such that with a dilation of value two, weights in a kernel are convolved with every second element of input time series | Dilation is sampled on an exponential scale d = ⌊2x⌋,x ∼ U(0,A), linput −1 where A = log2 lkernel −1 |\n| Padding | Padding involves appending values(typically zero) to the start and end of input time series such that the middle weight of a kernel aligns with the first value of input time series at start of convolution| When each kernel is generated, a decision is made (at random, with equal probability) whether or not padding will be used when applying the kernel| \n\n## Features generated by Rocket kernel\n\nRocket computes two aggregate features from each kernel and feature convolution. The two features are created using the well known methodology global/average max pooling and a unique methodology positive proportion value (ppv). \n\n### Max pooling\n\nGlobal max pooling is essentially picking the maximum value from the result of convolution and max pooling is picking the maximum value within a pool size. \nAssuming that the output of convolution is 0,1,2,2,5,1,2, global max pooling outputs 5, whereas ordinary max pooling  with pool size equals to 3 outputs 2,2,5,5,5\n\n### Proportion of positive values\n\nLet's try to understand using author's own words to describe ppv. \n\n> ppv directly captures the proportion of the input which matches a given pattern, i.e., for which the output of the convolution operation is positive. The ppv works in conjunction with the bias term. The bias term acts as a kind of ‘threshold’ for ppv. A positive bias value means that ppv captures the proportion of the input reflecting even ‘weak’ matches between the input and a given pattern, while a negative bias value means that ppv only captures the proportion of the input reflecting ‘strong’ matches between the input and the given pattern.\n\n## Rocket usage\n\nNow that we understand what kernels are and how rocket generates two outputs by convolution of kernel and input vector, let's understand how to use it.\n\nThe time series data needs to be provided as input into the rocket transform method, the value for number of kernels (i.e. k) is set at 10,000 by default. This means that if the input data has one feature then it would result in 20,000 features as output after rocket transform. \n\nThe tranformed feature table can now we used as any classification algorithm, authors advise linear algorithms like ridge regression classifier or logistic regression. \n\n\n## Rocket v/s others\n\nRocket's approach of creating large number of random karnels and generating two features is unique. Rocket distinguishes itself based on various factors which we will discuss below. \n\n### Rocket v/s neural nets\n\n1. Rocket doesn’t use a hidden layer or any non-linearities\n2. Features produced by Rocket are independent of each other\n3. Rocket works with any kind of classifier\n\n### Rocket v/s CNN\n\n1. Rocket uses very large number of kernels\n2. In CNN, a group of kernels tend to share same size, dilation and padding. Rocket has all 5 parameters randomized.\n3. In CNN, Dilation increases exponentially with depth; Rocket has random dilation values\n4. CNNs only have average/max pooling. Rocket has a unique pooling called as ppv which has proven to provide much better classification accuracy on time series. \n\n\n## Rocket performance\n\nAuthors provide detailed information about the classifictaion accuracy and time taken to train the model. I am discussing the results from bakeoff datasets but authors haave discussed about the results from various other datasets as well in the paper. \n\n### Accuracy\n\nRank is calculated by taking a mean value of classifictaion accuracy across all the 85 datasets in bakeoff datasets. \n\nIt is clear that the model trained using features derive using rocket are faring better compared to other models on average among all the datasets in bake off datasets. Please note that the dark horizontal line connecting the rank position of two models depict that the results from two mdoels are not statistically insignificant. Please read more about how the diagram is created here.\n\n### Time taken to train\n\n## Example\n\nIn the below examples, we are going to try and train a Human activity recogniser time series classifier.\nI am using two nice repo by Guillaume Chevalier showcasing [LSTM model on Human activity recogniser](https://github.com/jainds/LSTM-Human-Activity-Recognition ) with classifictaion accuracy of 91% and by [Thanatchon](https://github.com/thanatchon36/Rocket_vs_LSTM-Human-Activity-Recognition) . Let's see how much accuracy can be achieved by using rocket transforms. \n\nWe will be using sktime implementation of rocket in this example\n\n### Import Statements"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport os\nimport zipfile\n#orchestration\nfrom sklearn.pipeline import make_pipeline\n\n#transforms\nfrom sktime.transformers.series_as_features.rocket import Rocket \n\n#plot\nimport matplotlib.pyplot as plt\n\n#metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Classifier\nfrom sklearn.linear_model import RidgeClassifierCV","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download dataset and extract "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the file\n!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"","execution_count":2,"outputs":[{"output_type":"stream","text":"--2020-07-24 06:41:31--  https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 60999314 (58M) [application/x-httpd-php]\nSaving to: ‘UCI HAR Dataset.zip.1’\n\nUCI HAR Dataset.zip 100%[===================>]  58.17M  25.6MB/s    in 2.3s    \n\n2020-07-24 06:41:34 (25.6 MB/s) - ‘UCI HAR Dataset.zip.1’ saved [60999314/60999314]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract\n\nzip_ref = zipfile.ZipFile('UCI HAR Dataset.zip', 'r')\nzip_ref.extractall('./')\nzip_ref.close()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validate if file exists\n!ls ./'UCI HAR Dataset'","execution_count":4,"outputs":[{"output_type":"stream","text":"activity_labels.txt  features_info.txt\tfeatures.txt  README.txt  test\ttrain\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Useful Constants\n\n# Those are separate normalised input features for the neural network\nINPUT_SIGNAL_TYPES = [\n    \"body_acc_x_\",\n    \"body_acc_y_\",\n    \"body_acc_z_\",\n    \"body_gyro_x_\",\n    \"body_gyro_y_\",\n    \"body_gyro_z_\",\n    \"total_acc_x_\",\n    \"total_acc_y_\",\n    \"total_acc_z_\"\n]\n\n# Output classes to learn how to classify\nLABELS = [\n    \"WALKING\", \n    \"WALKING_UPSTAIRS\", \n    \"WALKING_DOWNSTAIRS\", \n    \"SITTING\", \n    \"STANDING\", \n    \"LAYING\"\n]","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining Train test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = \"./\"\nTRAIN = \"train/\"\nTEST = \"test/\"\nDATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n\n\n# Load \"X\" (the neural network's training and testing inputs)\n\ndef load_X(X_signals_paths):\n    X_signals = []\n    \n    for signal_type_path in X_signals_paths:\n        file = open(signal_type_path, 'r')\n        # Read dataset from disk, dealing with text files' syntax\n        X_signals.append(\n            [np.array(serie, dtype=np.float32) for serie in [\n                row.replace('  ', ' ').strip().split(' ') for row in file\n            ]]\n        )\n        file.close()\n    \n    return np.transpose(np.array(X_signals), (1, 2, 0))\n\nX_train_signals_paths = [\n    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n]\nX_test_signals_paths = [\n    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n]\n\nX_train = load_X(X_train_signals_paths)\nX_test = load_X(X_test_signals_paths)\n\n\n# Load \"y\" (the neural network's training and testing outputs)\n\ndef load_y(y_path):\n    file = open(y_path, 'r')\n    # Read dataset from disk, dealing with text file's syntax\n    y_ = np.array(\n        [elem for elem in [\n            row.replace('  ', ' ').strip().split(' ') for row in file\n        ]], \n        dtype=np.int32\n    )\n    file.close()\n    \n    # Substract 1 to each output class for friendly 0-based indexing \n    return y_ - 1\n\ny_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\ny_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n\ny_train = load_y(y_train_path)\ny_test = load_y(y_test_path)\n\n\nprint('OK !')","execution_count":6,"outputs":[{"output_type":"stream","text":"OK !\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Preparing dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(data_array):\n\n    dim_dict = {}\n\n    for i in range(9):\n        name_i = f'dim_{str(i)}'\n        dim_dict[name_i] = []\n\n    for i in range(data_array.shape[0]):\n        for j in range(data_array.shape[1]):\n            name_dim = f'dim_{str(j)}'\n            dim_dict[name_dim].append(pd.Series(data_array[i][j]).astype('float64'))\n            \n    return pd.DataFrame(dim_dict)\n\nprint('OK !')\n","execution_count":7,"outputs":[{"output_type":"stream","text":"OK !\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(-1,9,128)\nX_test = X_test.reshape(-1,9,128)\n\n%time X_train = preprocess_data(X_train)\n%time X_test = preprocess_data(X_test)\n\ny_train = list(y_train.copy().ravel())\ny_test = list(y_test.copy().ravel())\n\ny_train = [str(each) for each in y_train]\ny_test = [str(each) for each in y_test]","execution_count":8,"outputs":[{"output_type":"stream","text":"CPU times: user 15.5 s, sys: 657 ms, total: 16.1 s\nWall time: 16.1 s\nCPU times: user 5.83 s, sys: 337 ms, total: 6.17 s\nWall time: 6.18 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Rocket Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"rocket_pipeline = make_pipeline(\n    Rocket(num_kernels = 10000, random_state = 1),\n    RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n)\n\n%time rocket_pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy "},{"metadata":{"trusted":true},"cell_type":"code","source":"%time rocket_pipeline.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time y_pred = list(rocket_pipeline.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time plot_confusion_matrix(rocket_pipeline, X_test, y_test, display_labels = LABELS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nWe were able to improve upon accuracy achieved by LSTM model by a very simple implementation using Rocket. \nIt will suffice to say that Rocket based time series classification models is the kind of innovative, simple and fresh technique long awaited in the data science community. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}