{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, when building machine learning models, your goal is to provide insights to your business team or consumers about specific outcomes. The model offers an estimation or prediction to support the business outcome, but it doesn’t explain how to change that outcome. For instance, if you’ve worked with customer data, you might start by predicting which customers are likely to churn. However, the next logical question from the business would be: “What can we do with this information?”\n",
    "\n",
    "This leads to the second part of the problem: using predictions to inform actions. For example, if a customer is predicted to be high-risk, how can we reduce their risk? Conversely, if a customer is low-risk, how can we ensure they remain so? These scenarios highlight two types of interventions: reducing high-risk cases and proactively managing low-risk cases.\n",
    "\n",
    "Let’s delve into practical tools for this. I have been using SciKit-Learn as my go-to library for building regression and classification algorithms, and DiCE to generate counterfactuals. DiCE works well for datasets with a few thousand or tens of thousands of rows, but challenges arise with millions of rows. For instance, generating a counterfactual for a single row might take five seconds. Scaling this to 500 million rows would take approximately 500 million seconds, which is impractical.\n",
    "\n",
    "To address this, I’ve used Ray Serve to distribute the computation of counterfactuals across a Spark cluster. Ray allows you to leverage Spark nodes for parallel processing, even controlling how many CPU cores are used within each node. This setup significantly reduces computation time.\n",
    "\n",
    "Here are examples to illustrate the solution:\n",
    "\n",
    "## Example 1: Generating a Counterfactual for a Single Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from dice_ml import Dice\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `model` is your trained model and `data` is your input dataset\n",
    "dice = Dice(model=model, data=data, method=\"random\")\n",
    "\n",
    "# Generate a counterfactual\n",
    "counterfactual = dice.generate_counterfactuals(instance, total_CFs=1, desired_class=\"opposite\")\n",
    "print(counterfactual.cf_examples_list[0].final_cfs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Distributing Counterfactual Generation Using Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from ray.util.spark import setup_ray_cluster\n",
    "import ray\n",
    "\n",
    "def setup_ray_cluster_with_config(num_nodes, cpus_per_node):\n",
    "    # Initialize Ray on Spark cluster with configuration\n",
    "    setup_ray_cluster(num_nodes=num_nodes, cpus_per_node=cpus_per_node)\n",
    "\n",
    "# Example: Setting up Ray with specific parameters\n",
    "setup_ray_cluster_with_config(num_nodes=10, cpus_per_node=4)\n",
    "\n",
    "@ray.remote\n",
    "def generate_cf(row):\n",
    "    return dice.generate_counterfactuals(row, total_CFs=1, desired_class=\"opposite\")\n",
    "\n",
    "# Parallel processing\n",
    "results = ray.get([generate_cf.remote(row) for row in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using Broadcast Variables in Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Broadcasting model and data\n",
    "sc = SparkContext.getOrCreate()\n",
    "model_broadcast = sc.broadcast(model)\n",
    "data_broadcast = sc.broadcast(data)\n",
    "\n",
    "# Example function leveraging broadcast variables\n",
    "@ray.remote\n",
    "def process_row(row):\n",
    "    dice_local = Dice(model=model_broadcast.value, data=data_broadcast.value, method=\"random\")\n",
    "    return dice_local.generate_counterfactuals(row, total_CFs=1, desired_class=\"opposite\")\n",
    "\n",
    "results = ray.get([process_row.remote(row) for row in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, Ray supports logging mechanisms to collect logs from Spark nodes and propagate them to centralized logging systems, such as Log4j or custom log collectors. This ensures robust monitoring in production.\n",
    "\n",
    "In summary, we’ve covered:\n",
    "\n",
    "Setting up counterfactual analysis for classification problems.\n",
    "\n",
    "Distributing counterfactual generation using Ray on Spark clusters.\n",
    "\n",
    "Leveraging Ray’s broadcast features for efficient computation.\n",
    "\n",
    "Logging and monitoring distributed tasks in production.\n",
    "\n",
    "With these techniques, you can efficiently scale counterfactual analysis and integrate it into production systems. Thank you!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
