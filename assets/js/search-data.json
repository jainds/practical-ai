{
  
    
        "post0": {
            "title": "All you need to know about CVAT",
            "content": "Introduction . Once you start your journey into data science, you quickly learn that as part of your job you are spending more time with data than models. Data and labels go hand in hand and hence, I will be sharing what you need to know before you decide to use CVAT as your image/video annotation tool. Let’s start with the easier bit i.e. complete name of tool - Computer Vision Annotation Tool . Ease of Setup . A treat awaits in this section if you have worked with docker compose before, CVAT is very easy to setup as the source code contains necessary docker compose files which makes complete setup a breeze in your local. Repo also features an instruction documentation which is comprehensively written and covers every line of code that needs to be executed in order to get the tool running. . Complete setup is nicely segregated into docker-compose files with docker apps named cvat,cvat_ui, cvat_db. There are two ways to go ahead with setting up the tool, the easier way of deploying it in a VM and a little longer way on Kubernetes. A VM with enough firepower to run an instance of Postgres, Django backend and a react app to support the number of users that you expect to access the tool simultaneously should be alright. An example: I was able to support 200+ users(some of them were automated scripts) in a single VM with the analytics component running using a Standard D5 v2 (16 vcpus, 56 GiB memory) Azure VM. The power of your VM needs to be proportionately increased if you wish to use the additional components like deep learning model based auto labelling. If you choose to deploy the tool in Kubernetes, you could benefit from the auto-scaling functionality for each app inside CVAT. Kubernetes YAML files are not available as part of source code and hence you might need to create them yourself. I recommend Kompose to create Kubrnetes YAML from docker compose files. Scalability is an important aspect while considering a tool for production deployment. In order to make sure that the tool works for foreseeable growth in number of users. App can be scaled easily since everything is nicely wrapped into docker containers. . Ease of Usage . The application’s desktop UI although not fancy, is very feature rich and achieves the goal of labeling images and videos with ease.There’s a long list of keyboard shortcuts supported and you don’t necessarily need to remember every shortcut, simply pick and choose which help you speed up. I found the shortcut to create poly shapes and rectangles during labelling as very useful. Keyboard shortcuts combined with the feature rich app make labelling task slick and smooth. The application also features a task assignment and task process flow using which larger teams can collaborate by assigning tasks to a specific user and updating the current status of task for others to see. CVAT is developed considering the desktop based user interface, which means we need to keep expectations lower while trying to use it on mobile or tablets. You can try the app online right now by navigating to cvat.org CVAT also features a command line interface which enables you to perform simple CRUD operations on task. . Data Extraction/ Upload . A Django app in backend and react UI as frontend, CVAT is quite covered with options to upload data via UI or CLI. App works on a unique task based system, each upload is created as a task in the system. The task can then be further assigned to different users for labelling, quality check, etc. When using the UI to upload data, you don’t need to worry about label formatting because the app takes care of it. CLI based upload requires data to be structured in specific formats before it can be processed. I recommend CLI to perform automated scripts based data upload and UI when actual human is performing the uplaod and labelling task. . Extraction of data is a very important step which will have to be performed by every team on regular basis. CVAT supports extraction of data in a format by both interfaces i.e. UI and CLI. A user can go to a task and there’s option to extract the data in various supported formats. when trying to extract multiple data points or tasks in CVAT, UI based extraction might seem time consuming . CLI comes to rescue here, extraction of data in any supported format is super simple using CLI. An important thing to note here is CVAT currently doesn’t support bulk extraction or upload of data using UI. . The dev team also mentiond about datumaro dataset framework which can be used to transform, merge, extract multiple datasets from CVAT. I was not able to get it working and therefore no comments on that. . Annotations Format Supported . I am borrowing a table available in CVAT documentation to show the formats supported. It supports all major community defined data label formats. The labels covers the spectrum of classification, obejct detection and segmentation tasks in computer vision. . Annotation format Import Export . CVAT for images | X | X | . CVAT for a video | X | X | . Datumaro |   | X | . PASCAL VOC | X | X | . Segmentation masks from PASCAL VOC | X | X | . YOLO | X | X | . MS COCO Object Detection | X | X | . TFrecord | X | X | . MOT | X | X | . LabelMe 3.0 | X | X | . Backup and Restore . All images/videos uploaded are stored in docker volume cvat_data and the respective label data is stored in postgres. Postgres data is stored in docker volume cvat_db. In order to backup the complete app data, you can simply create volume backup for these two volumes in form of .tar files. Configuring the said docker volumes to a persistent storage like S3 or azure blob would enable you to setup automated cloud backup for these volumes. Restoration is as simple as backup by using docker commands. . Community . CVAT community is available on GitHub and Gitter. I have personally found them responding faster on gitter compared to raising issues on github. .",
            "url": "https://piyush.dev/annotation/2020/07/11/CVAT-All-you-need-to-know.html",
            "relUrl": "/annotation/2020/07/11/CVAT-All-you-need-to-know.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deployment Journey of a Reinforcement Learning Algorithm",
            "content": "Introduction . Our team is working to improve the health and wealth of millions of current customers and acquire more customers in the future. One of the most effective and efficient way to achieve our goal is by getting an app into the millions of people. As it turns out, we already have a wonderful application which is downloaded by more than 3 million users as I write this post. The mobile application has a carousel portion in the bottom half section of the home page where dynamic banners can be rendered. Each banner is utilized as a form of information, communication medium or an application feature. This is the first page that is seen by all users who successfully register and a portion of them clicking on the banner displayed registering their interest. Our team&#39;s goal is to increase engagement within the app. The first step was to understand the source of users who were clicking the banners, why are they willing to go into exploring app via banners after registering while others would go on to explore the app via other routes. . Problem statement . The goal was to increase user engagement within the app by understanding user’s interest in a variety of banners and then leverage the results across the app. . we didn’t have existing data about user interaction with the app neither did we have enough time at hand to perform that activity. We were also looking at an incoming huge inflow of new users expected in near future due to the planned marketing campaigns. We were essentially looking at a cold start problem to improve engagement since, we would know little about the new users and time to market was a very important factor. We were expected to go live within two weeks duration with a solution to make the best out of data available at hand. . Solution . Bayesian bandits with Thomson sampling ticked all boxes as follows: . It requires no data or less to start with compared to other options | It will learn incoming users/data and start recommending banners | Can work with new banners configured as new arms | The next phase of the project was also discussed where we agreed to work on building contextual bandits. In this post, I will be talking more about how we used various tools and technology making deployment possible. I will not be talking about how the recommendation algorithm works and the technology stack used to achieve it. . Deployment . . The build and deployment part of the project was broken down into two technical stages/phases: . Testing model and documenting results in the pre-prod environment with production data, define the input and the output schema for the model which will be used by the data engineer team to create a streaming pipeline. | Setup model to consume a live stream of event data, and respond via a REST endpoint with the recommended list of banners for the users | The front end of the mobile app is configured for a response time of one second w.r.t to back-end. It meant that the app will try to generate dynamic banners on the user screen based on our recommendations or fall back to static banners if we failed to deliver a response within a second, which added another layer of complexity to the second stage. Our APIs were expected to support a wide range of user load starting from a few hundred requests to millions across the region. . We could list the deployment infra into three major components: . A robust build and deployment pipeline | Automated performance testing | Production monitoring and alerting | Tools . Tools used for the complete setup: . Jenkins | Artifactory | Docker | Aquasec image scanning | Fortify static code scan 6.. Sonar Nexus open source code scanning | Kubernetes | Predator | Prometheus | Grafana | Bitbucket | Our application solution is a bunch of docker images which consumes/produces content in Kafka topics. . Step 1 &#8212; Fetching code and checking for changes . Our pipeline starts at fetching the code from Bitbucket repository. We store code in the folder structure for the 4 different docker images that are to be built. We check whether a file has been changed before initiating build for the files in that folder. The code in the Jenkins pipeline is as below for one of the folders titled ‘generator’. . #hide_output script{ GIT_RESULT = sh(script: &#39;&#39;&#39;git diff --quiet HEAD &quot;$(git rev-parse @~1)&quot; -- generator&#39;&#39;&#39;, returnStatus: true ) echo &quot;GIT RESULT -- ${GIT_RESULT} -- ${params.branchname}&quot; } . Step 2 - Fortify . Next step is to run complete code static security scanning by Fortify . #hide_output sh &#39;&#39;&#39; echo &quot;==================================================&quot; echo &quot;======== SAST - Fortify Scan: Start ========&quot; echo &quot;==================================================&quot; hostname whoami ls -ahl echo &#39;WORKSPACE: &#39; $WORKSPACE cd $WORKSPACE pwd sourceanalyzer -v sourceanalyzer -b ${fortify_app_name} -clean sourceanalyzer -b ${fortify_app_name} -python-version ${python_version} -python-path ${python_path} ${fortify_scan_files} sourceanalyzer -b ${fortify_app_name} -scan -f ${fortify_app_name}.fpr fortifyclient -url https://sast.intranet.asia/ssc -authtoken &quot;${fortify_upload_token}&quot; uploadFPR -file ${fortify_app_name}.fpr -project ${fortify_app_name} -version ${fortify_app_version} &#39;&#39;&#39; . Step 3 - Docker . The next step is to build the docker image. We first login to Artifactory before initiating the build as our pip libraries are also pulled from mirrored pip in the Artifactory. I have provided a sample of code on how we achieve this. . #hide_output sh &quot;&quot;&quot; echo ${ARTIFACTORY_PASSWORD} | docker login -u ${ARTIFACTORY_USERNAME} --password-stdin docker-registry:8443 cd generator docker build --file Docker-dev --build-arg HTTPS_PROXY=http://ip-address --build-arg ARTIFACTORY_USERNAME=${ARTIFACTORY_USERNAME} --build-arg ARTIFACTORY_PASSWORD=${ARTIFACTORY_PASSWORD} -t ${env.generator_image_latest} . docker tag ${env.generator_image_latest} ${env.generator_image_name} docker push ${env.generator_image_latest} docker push ${env.generator_image_name} docker logout docker-pcaaicoe.pruregistry.intranet.asia:8443 cd .. &quot;&quot;&quot; . Step 4 - Aquasec . After pushing an image into Artifactory, the next important and mandatory step to be performed is docker image security scanning. . #hide_output sh &quot;&quot;&quot; echo &quot;==================================================&quot; echo &quot;============= OSS - Nexus Scan =============&quot; echo &quot;==================================================&quot; docker save -o generator-dev.tar ${env.generator_image_latest} &quot;&quot;&quot; String result = nexusscan(&quot;pcaaicoeaipulsenudgesgeneratordev&quot;, &quot;$WORKSPACE&quot;, &quot;build&quot;); echo result; sh &quot;&quot;&quot; rm -f generator-dev.tar &quot;&quot;&quot; sh &quot;&quot;&quot; echo &quot;==================================================&quot; echo &quot;============= CSEC - Aquasec Scan ==========&quot; echo &quot;==================================================&quot; &quot;&quot;&quot; aquasecscan(&quot;${env.generator_image_latest}&quot;) . The code and image security scanning stages are major milestones to be cleared during the deployment phase. It is important as well as difficult to explain and agree between application security teams about what risks are we willing to take while allowing open source libraries with bugs to go live in our environment. . Step 5 &#8212; Kubernetes . Now we move on to the stage where we will be able to actually deploy and run our images. In order to deploy our solution, we need a Redis DB and Kafka cluster up and running. We deploy our docker images using the below code: . #hide_output sh &#39;&#39;&#39; set +x echo &quot;- preparing options -&quot; export HTTPS_PROXY=ip-address:8080 export KUBE_NAMESPACE=&quot;internal-namespace&quot; export KC_OPTS=${KC_OPTS}&quot; --kubeconfig=${KUBE_CONFIG}&quot; export KC_OPTS=${KC_OPTS}&quot; --insecure-skip-tls-verify=true&quot; export KC_OPTS=${KC_OPTS}&quot; --namespace=${KUBE_NAMESPACE}&quot; echo &quot;- prepared options -&quot; echo &quot;- preparing alias -&quot; alias kc=&quot;kubectl ${KC_OPTS} $*&quot; echo &quot;- alias prepared -&quot; echo &quot;- applying manifest -&quot; kc apply -f configmap.yaml if [ $which_app = &quot;generator&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-events-reader||echo fi sed -i &quot;s!GENERATOR_VERSION!$image_version!g&quot; &quot;generator.yaml&quot; kc apply -f generator.yaml fi if [ $which_app = &quot;aggregator&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-click-counter||echo fi sed -i &quot;s!AGGREGATOR_VERSION!$image_version!g&quot; &quot;aggregator.yaml&quot; kc apply -f aggregator.yaml fi if [ $which_app = &quot;detector&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-engine||echo fi sed -i &quot;s!DETECTOR_VERSION!$image_version!g&quot; &quot;detector.yaml&quot; kc apply -f detector.yaml fi if [ $which_app = &quot;restapi&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-restapi||echo fi sed -i &quot;s!REST_VERSION!$image_version!g&quot; &quot;restapi.yaml&quot; kc apply -f restapi.yaml fi if [ $which_app = &quot;all&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-events-reader||echo kc delete deploy ai-pulse-nudges-click-counter||echo kc delete deploy ai-pulse-nudges-engine||echo kc delete deploy ai-pulse-nudges-restapi||echo fi sed -i &quot;s!GENERATOR_VERSION!$image_version!g&quot; &quot;generator.yaml&quot; sed -i &quot;s!AGGREGATOR_VERSION!$image_version!g&quot; &quot;aggregator.yaml&quot; sed -i &quot;s!DETECTOR_VERSION!$image_version!g&quot; &quot;detector.yaml&quot; sed -i &quot;s!REST_VERSION!$image_version!g&quot; &quot;restapi.yaml&quot; kc apply -f generator.yaml kc apply -f aggregator.yaml kc apply -f detector.yaml kc apply -f restapi.yaml fi echo &quot;- manifest applied -&quot; echo &quot;- checking result -&quot; echo &quot; &gt;&gt; Deployments &quot; kc get deployments echo &quot; &gt;&gt; Services&quot; kc get svc echo &quot; &gt;&gt; Ingress&quot; kc get ingress echo &quot; &gt;&gt; Pods&quot; kc get pods echo &quot;- Done -&quot; &#39;&#39;&#39; . Step 6 &#8212; Performance test . We deploy Predator — the tool which we use for performance test. . #hide_output sh &#39;&#39;&#39; set +x echo &quot;- preparing options -&quot; export HTTPS_PROXY=ip-address:8080 export KUBE_NAMESPACE=&quot;internal-namespace&quot; export KC_OPTS=${KC_OPTS}&quot; --kubeconfig=${KUBE_CONFIG}&quot; export KC_OPTS=${KC_OPTS}&quot; --insecure-skip-tls-verify=true&quot; export KC_OPTS=${KC_OPTS}&quot; --namespace=${KUBE_NAMESPACE}&quot; echo &quot;- prepared options -&quot; echo &quot;- preparing alias -&quot; alias kc=&quot;kubectl ${KC_OPTS} $*&quot; echo &quot;- alias prepared -&quot; echo &quot;- applying manifest -&quot; kc get deploy|grep predator|awk &#39;{print $1 }&#39; || echo kc get deploy|grep predator|awk &#39;{print $1 }&#39;|xargs kc delete deploy || echo for i in `seq $replica_count` do echo $i cp -rf predator/predator.yaml tmp.yaml sed -i &quot;s!REPLICA_NO!$i&quot;&quot;!g&quot; &quot;tmp.yaml&quot; kc apply -f tmp.yaml done # kc apply -f predator/predator.yaml echo &quot;- manifest applied -&quot; echo &quot;- checking result -&quot; echo &quot; &gt;&gt; Deployments &quot; kc get deployments echo &quot; &gt;&gt; Services&quot; kc get svc echo &quot; &gt;&gt; Ingress&quot; kc get ingress echo &quot; &gt;&gt; Pods&quot; kc get pods echo &quot;- Done -&quot; &#39;&#39;&#39; . Predator is an amazing tool that enables us to leverage existing Kubernetes infra for an unlimited number of users for testing. Read more about the tool here: https://medium.com/zooz-engineering/by-niv-lipetz-software-engineer-zooz-b5928da0b7a8 We leverage the existing enterprise Prometheus and Grafana set up to monitor the application pods. . Lessons learned for next time: . We started writing the pipeline code from scratch, whereas it would have helped save time if an advanced hello world type of empty pipeline existed, which could be used as a template structure. It would have enabled us to know what credentials and access were required at what stage. | There were many credentials and access that were required to get the pipeline up and running. It would be a time and effort savior if we have one master service id created and assigned to a pipeline which can then be used across all tools in the organization. | It is very difficult to build a machine learning model, and real-time streaming data was an additional complexity, but productionizing that model with streaming data is many folds difficult. | Contributors . Glenn Bayne, Tien Nguyet Long, John Yue, Zeldon Tay （郑育忠), Steven Chau , Denys Pang , Philipp Gschoepf , Syam Bandi , Uma Maheshwari, Michael Natusch .",
            "url": "https://piyush.dev/deployment/2020/07/07/Deploying-RL-Model.html",
            "relUrl": "/deployment/2020/07/07/Deploying-RL-Model.html",
            "date": " • Jul 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, Welcome to Piyush.Dev . I am a data scientist who enjoys playing with new technologies and identify possible use cases in the real world. . I am an engineer by qualification, turned to data science by sheer nature of curiosity. . I like learning new things and sharing here. . Welcome to my site. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://piyush.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://piyush.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}