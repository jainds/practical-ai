{
  
    
        "post0": {
            "title": "Random Convolutional Kernel Transform",
            "content": "Introduction . ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels is a reasearch paper published in October 2019 by Angus Dempster, François Petitjean, Geoffrey I. Webb. The paper presents a unique methodology to transform time series data using convolutional kernels in order to improve classification accuracy. This paper is unique in learning from recent success of convolutional neural networks and transferring it on time series datasets. . The link to download the paper from arxiv - Paper . Time Series data . Time series data is defined as set of data points containing details about different point in time. Generally time series data contains data points sampled/observed at equal interval of time. Time series classification can be imagined as identifying patterns and signals in time series data in relation to respective classes. . Authors in this paper are promising fast and accurate time series classification. The proposal is features generated by convolution of randomly generated kernels on time series data results in good accuracy. We will go into more details of this proposal and understand how methodology proposed by aither&#39;s helps to improve the accuracy. . Kernels . Kernels in simple terms is a small matrix used to modify the images. Let&#39;s try to understand kernels using an example: . here is a 3 x 3 kernel used to sharpen images: . $ begin{bmatrix} 0 &amp; -1 &amp; 0 -1 &amp; 5 &amp; -1 0 &amp; -1 &amp; 0 end{bmatrix}$ . In order to sharpen an image using above kernel, we need to perform a dot product of each pixel in image with the kernel matrix. The resulting image would then be a sharpened version of original image. Observe the gif below to see a live version of kernel dot product in motion. . Following is an example from setosa.io site to demonstrate how kernels can change the images. . . 5 parameters of kernels . A kernel has 5 different parameter using which it can be configured. . Parameter Description Value logic . Bias | Bias is added to the result of the convolution operation between input time series and weights of the given kernel | Bias is sampled from a uniform distribution, b ∼ U(−1,1) | . Size(Length) | Size defines the number of rows and columns a kernel has. The above example has a size of 3 rows and 3 colums | Length is selected randomly from {7,9,11} with equal probability, making kernels considerably shorter than input time series in most cases | . Weights | The values that make up the kernel matrix are weights | The weights are sampled from a normal distribution, ∀w ∈ W, w ∼ N(0,1), and are mean centered after being set, ω = W − W. As such, most weights are relatively small, but can take on larger magnitudes | . Dilation | Dilation spreads a kernel over the input such that with a dilation of value two, weights in a kernel are convolved with every second element of input time series | Dilation is sampled on an exponential scale d = ⌊2x⌋,x ∼ U(0,A), linput −1 where A = log2 lkernel −1 | . Padding | Padding involves appending values(typically zero) to the start and end of input time series such that the middle weight of a kernel aligns with the first value of input time series at start of convolution | When each kernel is generated, a decision is made (at random, with equal probability) whether or not padding will be used when applying the kernel | . Features generated by Rocket kernel . Rocket computes two aggregate features from each kernel and feature convolution. The two features are created using the well known methodology global/average max pooling and a unique methodology positive proportion value (ppv). . Max pooling . Global max pooling is essentially picking the maximum value from the result of convolution and max pooling is picking the maximum value within a pool size. Assuming that the output of convolution is 0,1,2,2,5,1,2, global max pooling outputs 5, whereas ordinary max pooling with pool size equals to 3 outputs 2,2,5,5,5 . Proportion of positive values . Let&#39;s try to understand using author&#39;s own words to describe ppv. . ppv directly captures the proportion of the input which matches a given pattern, i.e., for which the output of the convolution operation is positive. The ppv works in conjunction with the bias term. The bias term acts as a kind of ‘threshold’ for ppv. A positive bias value means that ppv captures the proportion of the input reflecting even ‘weak’ matches between the input and a given pattern, while a negative bias value means that ppv only captures the proportion of the input reflecting ‘strong’ matches between the input and the given pattern. . Rocket usage . Now that we understand what kernels are and how rocket generates two outputs by convolution of kernel and input vector, let&#39;s understand how to use it. . The time series data needs to be provided as input into the rocket transform method, the value for number of kernels (i.e. k) is set at 10,000 by default. This means that for every one input feature it would result in 20,000 features as output after rocket transform. . The tranformed feature table can now we used as input data for any classification algorithm, authors advise linear algorithms like ridge regression classifier or logistic regression. . Rocket v/s others . Rocket&#39;s approach of creating large number of random karnels and generating two features is unique. Rocket distinguishes itself based on various factors which we will discuss below. . Rocket v/s neural nets . Rocket doesn’t use a hidden layer or any non-linearities | Features produced by Rocket are independent of each other | Rocket works with any kind of classifier | Rocket v/s CNN . Rocket uses very large number of kernels | In CNN, a group of kernels tend to share same size, dilation and padding. Rocket has all 5 parameters randomized. | In CNN, Dilation increases exponentially with depth; Rocket has random dilation values | CNNs only have average/max pooling. Rocket has a unique pooling called as ppv which has proven to provide much better classification accuracy on time series. | Rocket performance . Authors provide detailed information about the classifictaion accuracy and time taken to train the model. I am discussing the results from bakeoff datasets but authors haave discussed about the results from various other datasets as well in the paper. . Accuracy . Rank is calculated by taking a mean value of classifictaion accuracy across all the 85 datasets in bakeoff datasets. . It is clear that the model trained using features derive using rocket are faring better compared to other models on average among all the datasets in bake off datasets. Please note that the dark horizontal line connecting the rank position of two models depict that the results from two mdoels are not statistically insignificant. . . Time taken to train . Architecture Largest dataset(ElectricDevices, with 8,926 training examples) Longest time series(HandOutlines, with time series of length 2,709) . ROCKET | 6 minutes 33 seconds | 4 minutes 18 seconds | . MrSEQL | 31 minutes | 1 hour 55 minutes | . cBOSS | 3 hours 6 minutes | 42 minutes | . Proximity Forest | 1 hour 35 minutes | 3 days | . TS-CHIEF | 2 hours 24 minutes | 4 days | . Inception Time (on GPU) | 7 hours 46 minutes | 8 hours 10 minutes | . Example . In the below examples, we are going to try and train a Human activity recogniser time series classifier. I am using two nice repo by Guillaume Chevalier showcasing LSTM model on Human activity recogniser with classifictaion accuracy of 91% and by Thanatchon . Let&#39;s see how much accuracy can be achieved by using rocket transforms. . We will be using sktime implementation of rocket in this example . Install libraries &amp; import Statements . #collapse-hide import pandas as pd import numpy as np from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; import os import zipfile #orchestration from sklearn.pipeline import make_pipeline #transforms from sktime.transformers.series_as_features.rocket import Rocket #plot import matplotlib.pyplot as plt #metrics from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.metrics import plot_confusion_matrix # Classifier from sklearn.linear_model import RidgeClassifierCV . . Download dataset and extract . #collapse-hide # Download the file !wget &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip&quot; . . --2020-07-24 06:58:35-- https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 60999314 (58M) [application/x-httpd-php] Saving to: ‘UCI HAR Dataset.zip’ UCI HAR Dataset.zip 100%[===================&gt;] 58.17M 35.0MB/s in 1.7s 2020-07-24 06:58:37 (35.0 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314] . #collapse-hide # Extract zip_ref = zipfile.ZipFile(&#39;UCI HAR Dataset.zip&#39;, &#39;r&#39;) zip_ref.extractall(&#39;./&#39;) zip_ref.close() . . #collapse-hide # validate if file exists !ls ./&#39;UCI HAR Dataset&#39; . . activity_labels.txt features_info.txt features.txt README.txt test train . #collapse-hide # Useful Constants # Those are separate normalised input features for the neural network INPUT_SIGNAL_TYPES = [ &quot;body_acc_x_&quot;, &quot;body_acc_y_&quot;, &quot;body_acc_z_&quot;, &quot;body_gyro_x_&quot;, &quot;body_gyro_y_&quot;, &quot;body_gyro_z_&quot;, &quot;total_acc_x_&quot;, &quot;total_acc_y_&quot;, &quot;total_acc_z_&quot; ] # Output classes to learn how to classify LABELS = [ &quot;WALKING&quot;, &quot;WALKING_UPSTAIRS&quot;, &quot;WALKING_DOWNSTAIRS&quot;, &quot;SITTING&quot;, &quot;STANDING&quot;, &quot;LAYING&quot; ] . . Defining Train test data . #collapse-show DATA_PATH = &quot;./&quot; TRAIN = &quot;train/&quot; TEST = &quot;test/&quot; DATASET_PATH = DATA_PATH + &quot;UCI HAR Dataset/&quot; # Load &quot;X&quot; (the neural network&#39;s training and testing inputs) def load_X(X_signals_paths): X_signals = [] for signal_type_path in X_signals_paths: file = open(signal_type_path, &#39;r&#39;) # Read dataset from disk, dealing with text files&#39; syntax X_signals.append( [np.array(serie, dtype=np.float32) for serie in [ row.replace(&#39; &#39;, &#39; &#39;).strip().split(&#39; &#39;) for row in file ]] ) file.close() return np.transpose(np.array(X_signals), (1, 2, 0)) X_train_signals_paths = [ DATASET_PATH + TRAIN + &quot;Inertial Signals/&quot; + signal + &quot;train.txt&quot; for signal in INPUT_SIGNAL_TYPES ] X_test_signals_paths = [ DATASET_PATH + TEST + &quot;Inertial Signals/&quot; + signal + &quot;test.txt&quot; for signal in INPUT_SIGNAL_TYPES ] X_train = load_X(X_train_signals_paths) X_test = load_X(X_test_signals_paths) # Load &quot;y&quot; (the neural network&#39;s training and testing outputs) def load_y(y_path): file = open(y_path, &#39;r&#39;) # Read dataset from disk, dealing with text file&#39;s syntax y_ = np.array( [elem for elem in [ row.replace(&#39; &#39;, &#39; &#39;).strip().split(&#39; &#39;) for row in file ]], dtype=np.int32 ) file.close() # Substract 1 to each output class for friendly 0-based indexing return y_ - 1 y_train_path = DATASET_PATH + TRAIN + &quot;y_train.txt&quot; y_test_path = DATASET_PATH + TEST + &quot;y_test.txt&quot; y_train = load_y(y_train_path) y_test = load_y(y_test_path) print(&#39;OK !&#39;) . . OK ! . Preparing dataset . #collapse-show def preprocess_data(data_array): dim_dict = {} for i in range(9): name_i = f&#39;dim_{str(i)}&#39; dim_dict[name_i] = [] for i in range(data_array.shape[0]): for j in range(data_array.shape[1]): name_dim = f&#39;dim_{str(j)}&#39; dim_dict[name_dim].append( pd.Series(data_array[i][j]).astype(&#39;float64&#39;)) return pd.DataFrame(dim_dict) print(&#39;OK !&#39;) . . OK ! . #collapse-show X_train = X_train.reshape(-1,9,128) X_test = X_test.reshape(-1,9,128) %time X_train = preprocess_data(X_train) %time X_test = preprocess_data(X_test) y_train = list(y_train.copy().ravel()) y_test = list(y_test.copy().ravel()) y_train = [str(each) for each in y_train] y_test = [str(each) for each in y_test] . . CPU times: user 15.6 s, sys: 382 ms, total: 16 s Wall time: 15.5 s CPU times: user 6.62 s, sys: 282 ms, total: 6.9 s Wall time: 6.47 s . Rocket Model . rocket_pipeline = make_pipeline( Rocket(num_kernels = 10000, random_state = 1), RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True) ) %time rocket_pipeline.fit(X_train, y_train) . CPU times: user 15min 33s, sys: 8.24 s, total: 15min 41s Wall time: 8min 4s . Pipeline(memory=None, steps=[(&#39;rocket&#39;, Rocket(normalise=True, num_kernels=10000, random_state=1)), (&#39;ridgeclassifiercv&#39;, RidgeClassifierCV(alphas=array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01, 4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01, 2.15443469e+02, 1.00000000e+03]), class_weight=None, cv=None, fit_intercept=True, normalize=True, scoring=None, store_cv_values=False))], verbose=False) . Accuracy . %time rocket_pipeline.score(X_test, y_test) . CPU times: user 4min 32s, sys: 193 ms, total: 4min 32s Wall time: 2min 17s . 0.9355276552426196 . %time y_pred = list(rocket_pipeline.predict(X_test)) . CPU times: user 4min 32s, sys: 128 ms, total: 4min 32s Wall time: 2min 17s . %time plot_confusion_matrix(rocket_pipeline, X_test, y_test, display_labels = LABELS) . CPU times: user 4min 32s, sys: 247 ms, total: 4min 32s Wall time: 2min 18s . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f49fc711cf8&gt; . Conclusion . We were able to improve upon accuracy achieved by LSTM model by a very simple implementation using Rocket. The performance gain achieved becomes sweeter when you put the time required to code and train into consideration, which was very small comapred to LSTM in our case. The Rocket methodlogy is an innovative, simple and fresh technique that attracted my attention to this reserach paper. .",
            "url": "https://piyush.dev/research%20paper/time%20series/classification/2020/07/24/Random-convolutional-kernel-transform.html",
            "relUrl": "/research%20paper/time%20series/classification/2020/07/24/Random-convolutional-kernel-transform.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Random Convolutional Kernel Transform",
            "content": "Introduction . ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels is a reasearch paper published in October 2019 by Angus Dempster, François Petitjean, Geoffrey I. Webb. The paper presents a unique methodology to transform time series data using convolutional kernels in order to improve classification accuracy. This paper is unique in learning from recent success of convolutional neural networks and transferring it on time series datasets. . The link to download the paper from arxiv - Paper . Time Series data . Time series data is defined as set of data points containing details about different point in time. Generally time series data contains data points sampled/observed at equal interval of time. Time series classification can be imagined as identifying patterns and signals in time series data in relation to respective classes. . Authors in this paper are promising fast and accurate time series classification. The proposal is features generated by convolution of randomly generated kernels on time series data results in good accuracy. We will go into more details of this proposal and understand how methodology proposed by aither&#39;s helps to improve the accuracy. . Kernels . Kernels in simple terms is a small matrix used to modify the images. Let&#39;s try to understand kernels using an example: . here is a 3 x 3 kernel used to sharpen images: . $ begin{bmatrix} 0 &amp; -1 &amp; 0 -1 &amp; 5 &amp; -1 0 &amp; -1 &amp; 0 end{bmatrix}$ . In order to sharpen an image using above kernel, we need to perform a dot product of each pixel in image with the kernel matrix. The resulting image would then be a sharpened version of original image. Observe the gif below to see a live version of kernel dot product in motion. . Following is an example from setosa.io site to demonstrate how kernels can change the images. . . 5 parameters of kernels . A kernel has 5 different parameter using which it can be configured. . Parameter Description Value logic . Bias | Bias is added to the result of the convolution operation between input time series and weights of the given kernel | Bias is sampled from a uniform distribution, b ∼ U(−1,1) | . Size(Length) | Size defines the number of rows and columns a kernel has. The above example has a size of 3 rows and 3 colums | Length is selected randomly from {7,9,11} with equal probability, making kernels considerably shorter than input time series in most cases | . Weights | The values that make up the kernel matrix are weights | The weights are sampled from a normal distribution, ∀w ∈ W, w ∼ N(0,1), and are mean centered after being set, ω = W − W. As such, most weights are relatively small, but can take on larger magnitudes | . Dilation | Dilation spreads a kernel over the input such that with a dilation of value two, weights in a kernel are convolved with every second element of input time series | Dilation is sampled on an exponential scale d = ⌊2x⌋,x ∼ U(0,A), linput −1 where A = log2 lkernel −1 | . Padding | Padding involves appending values(typically zero) to the start and end of input time series such that the middle weight of a kernel aligns with the first value of input time series at start of convolution | When each kernel is generated, a decision is made (at random, with equal probability) whether or not padding will be used when applying the kernel | . Features generated by Rocket kernel . Rocket computes two aggregate features from each kernel and feature convolution. The two features are created using the well known methodology global/average max pooling and a unique methodology positive proportion value (ppv). . Max pooling . Global max pooling is essentially picking the maximum value from the result of convolution and max pooling is picking the maximum value within a pool size. Assuming that the output of convolution is 0,1,2,2,5,1,2, global max pooling outputs 5, whereas ordinary max pooling with pool size equals to 3 outputs 2,2,5,5,5 . Proportion of positive values . Let&#39;s try to understand using author&#39;s own words to describe ppv. . ppv directly captures the proportion of the input which matches a given pattern, i.e., for which the output of the convolution operation is positive. The ppv works in conjunction with the bias term. The bias term acts as a kind of ‘threshold’ for ppv. A positive bias value means that ppv captures the proportion of the input reflecting even ‘weak’ matches between the input and a given pattern, while a negative bias value means that ppv only captures the proportion of the input reflecting ‘strong’ matches between the input and the given pattern. . Rocket usage . Now that we understand what kernels are and how rocket generates two outputs by convolution of kernel and input vector, let&#39;s understand how to use it. . The time series data needs to be provided as input into the rocket transform method, the value for number of kernels (i.e. k) is set at 10,000 by default. This means that for every one input feature it would result in 20,000 features as output after rocket transform. . The tranformed feature table can now we used as input data for any classification algorithm, authors advise linear algorithms like ridge regression classifier or logistic regression. . Rocket v/s others . Rocket&#39;s approach of creating large number of random karnels and generating two features is unique. Rocket distinguishes itself based on various factors which we will discuss below. . Rocket v/s neural nets . Rocket doesn’t use a hidden layer or any non-linearities | Features produced by Rocket are independent of each other | Rocket works with any kind of classifier | Rocket v/s CNN . Rocket uses very large number of kernels | In CNN, a group of kernels tend to share same size, dilation and padding. Rocket has all 5 parameters randomized. | In CNN, Dilation increases exponentially with depth; Rocket has random dilation values | CNNs only have average/max pooling. Rocket has a unique pooling called as ppv which has proven to provide much better classification accuracy on time series. | Rocket performance . Authors provide detailed information about the classifictaion accuracy and time taken to train the model. I am discussing the results from bakeoff datasets but authors haave discussed about the results from various other datasets as well in the paper. . Accuracy . Rank is calculated by taking a mean value of classifictaion accuracy across all the 85 datasets in bakeoff datasets. . It is clear that the model trained using features derive using rocket are faring better compared to other models on average among all the datasets in bake off datasets. Please note that the dark horizontal line connecting the rank position of two models depict that the results from two mdoels are not statistically insignificant. . . Time taken to train . Architecture Largest dataset(ElectricDevices, with 8,926 training examples) Longest time series(HandOutlines, with time series of length 2,709) . ROCKET | 6 minutes 33 seconds | 4 minutes 18 seconds | . MrSEQL | 31 minutes | 1 hour 55 minutes | . cBOSS | 3 hours 6 minutes | 42 minutes | . Proximity Forest | 1 hour 35 minutes | 3 days | . TS-CHIEF | 2 hours 24 minutes | 4 days | . Inception Time (on GPU) | 7 hours 46 minutes | 8 hours 10 minutes | . Example . In the below examples, we are going to try and train a Human activity recogniser time series classifier. I am using two nice repo by Guillaume Chevalier showcasing LSTM model on Human activity recogniser with classifictaion accuracy of 91% and by Thanatchon . Let&#39;s see how much accuracy can be achieved by using rocket transforms. . We will be using sktime implementation of rocket in this example . Install libraries &amp; import Statements . #collapse-hide import pandas as pd import numpy as np from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; import os import zipfile #orchestration from sklearn.pipeline import make_pipeline #transforms from sktime.transformers.series_as_features.rocket import Rocket #plot import matplotlib.pyplot as plt #metrics from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.metrics import plot_confusion_matrix # Classifier from sklearn.linear_model import RidgeClassifierCV . . Download dataset and extract . #collapse-hide # Download the file !wget &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip&quot; . . --2020-07-24 06:58:35-- https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 60999314 (58M) [application/x-httpd-php] Saving to: ‘UCI HAR Dataset.zip’ UCI HAR Dataset.zip 100%[===================&gt;] 58.17M 35.0MB/s in 1.7s 2020-07-24 06:58:37 (35.0 MB/s) - ‘UCI HAR Dataset.zip’ saved [60999314/60999314] . #collapse-hide # Extract zip_ref = zipfile.ZipFile(&#39;UCI HAR Dataset.zip&#39;, &#39;r&#39;) zip_ref.extractall(&#39;./&#39;) zip_ref.close() . . #collapse-hide # validate if file exists !ls ./&#39;UCI HAR Dataset&#39; . . activity_labels.txt features_info.txt features.txt README.txt test train . #collapse-hide # Useful Constants # Those are separate normalised input features for the neural network INPUT_SIGNAL_TYPES = [ &quot;body_acc_x_&quot;, &quot;body_acc_y_&quot;, &quot;body_acc_z_&quot;, &quot;body_gyro_x_&quot;, &quot;body_gyro_y_&quot;, &quot;body_gyro_z_&quot;, &quot;total_acc_x_&quot;, &quot;total_acc_y_&quot;, &quot;total_acc_z_&quot; ] # Output classes to learn how to classify LABELS = [ &quot;WALKING&quot;, &quot;WALKING_UPSTAIRS&quot;, &quot;WALKING_DOWNSTAIRS&quot;, &quot;SITTING&quot;, &quot;STANDING&quot;, &quot;LAYING&quot; ] . . Defining Train test data . #collapse-show DATA_PATH = &quot;./&quot; TRAIN = &quot;train/&quot; TEST = &quot;test/&quot; DATASET_PATH = DATA_PATH + &quot;UCI HAR Dataset/&quot; # Load &quot;X&quot; (the neural network&#39;s training and testing inputs) def load_X(X_signals_paths): X_signals = [] for signal_type_path in X_signals_paths: file = open(signal_type_path, &#39;r&#39;) # Read dataset from disk, dealing with text files&#39; syntax X_signals.append( [np.array(serie, dtype=np.float32) for serie in [ row.replace(&#39; &#39;, &#39; &#39;).strip().split(&#39; &#39;) for row in file ]] ) file.close() return np.transpose(np.array(X_signals), (1, 2, 0)) X_train_signals_paths = [ DATASET_PATH + TRAIN + &quot;Inertial Signals/&quot; + signal + &quot;train.txt&quot; for signal in INPUT_SIGNAL_TYPES ] X_test_signals_paths = [ DATASET_PATH + TEST + &quot;Inertial Signals/&quot; + signal + &quot;test.txt&quot; for signal in INPUT_SIGNAL_TYPES ] X_train = load_X(X_train_signals_paths) X_test = load_X(X_test_signals_paths) # Load &quot;y&quot; (the neural network&#39;s training and testing outputs) def load_y(y_path): file = open(y_path, &#39;r&#39;) # Read dataset from disk, dealing with text file&#39;s syntax y_ = np.array( [elem for elem in [ row.replace(&#39; &#39;, &#39; &#39;).strip().split(&#39; &#39;) for row in file ]], dtype=np.int32 ) file.close() # Substract 1 to each output class for friendly 0-based indexing return y_ - 1 y_train_path = DATASET_PATH + TRAIN + &quot;y_train.txt&quot; y_test_path = DATASET_PATH + TEST + &quot;y_test.txt&quot; y_train = load_y(y_train_path) y_test = load_y(y_test_path) print(&#39;OK !&#39;) . . OK ! . Preparing dataset . #collapse-show def preprocess_data(data_array): dim_dict = {} for i in range(9): name_i = f&#39;dim_{str(i)}&#39; dim_dict[name_i] = [] for i in range(data_array.shape[0]): for j in range(data_array.shape[1]): name_dim = f&#39;dim_{str(j)}&#39; dim_dict[name_dim].append( pd.Series(data_array[i][j]).astype(&#39;float64&#39;)) return pd.DataFrame(dim_dict) print(&#39;OK !&#39;) . . OK ! . #collapse-show X_train = X_train.reshape(-1,9,128) X_test = X_test.reshape(-1,9,128) %time X_train = preprocess_data(X_train) %time X_test = preprocess_data(X_test) y_train = list(y_train.copy().ravel()) y_test = list(y_test.copy().ravel()) y_train = [str(each) for each in y_train] y_test = [str(each) for each in y_test] . . CPU times: user 15.6 s, sys: 382 ms, total: 16 s Wall time: 15.5 s CPU times: user 6.62 s, sys: 282 ms, total: 6.9 s Wall time: 6.47 s . Rocket Model . rocket_pipeline = make_pipeline( Rocket(num_kernels = 10000, random_state = 1), RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True) ) %time rocket_pipeline.fit(X_train, y_train) . CPU times: user 15min 33s, sys: 8.24 s, total: 15min 41s Wall time: 8min 4s . Pipeline(memory=None, steps=[(&#39;rocket&#39;, Rocket(normalise=True, num_kernels=10000, random_state=1)), (&#39;ridgeclassifiercv&#39;, RidgeClassifierCV(alphas=array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01, 4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01, 2.15443469e+02, 1.00000000e+03]), class_weight=None, cv=None, fit_intercept=True, normalize=True, scoring=None, store_cv_values=False))], verbose=False) . Accuracy . %time rocket_pipeline.score(X_test, y_test) . CPU times: user 4min 32s, sys: 193 ms, total: 4min 32s Wall time: 2min 17s . 0.9355276552426196 . %time y_pred = list(rocket_pipeline.predict(X_test)) . CPU times: user 4min 32s, sys: 128 ms, total: 4min 32s Wall time: 2min 17s . %time plot_confusion_matrix(rocket_pipeline, X_test, y_test, display_labels = LABELS) . CPU times: user 4min 32s, sys: 247 ms, total: 4min 32s Wall time: 2min 18s . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f49fc711cf8&gt; . Conclusion . We were able to improve upon accuracy achieved by LSTM model by a very simple implementation using Rocket. The performance gain achieved becomes sweeter when you put the time required to code and train into consideration, which was very small comapred to LSTM in our case. The Rocket methodlogy is an innovative, simple and fresh technique that attracted my attention to this reserach paper. .",
            "url": "https://piyush.dev/research%20paper/time%20series/classification/2020/07/23/Random-convolutional-kernel-transform.html",
            "relUrl": "/research%20paper/time%20series/classification/2020/07/23/Random-convolutional-kernel-transform.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "All you need to know about CVAT",
            "content": "Introduction . Once you start your journey into data science, you quickly learn that as part of your job you are spending more time with data than models. Data and labels go hand in hand and hence, I will be sharing what you need to know before you decide to use CVAT as your image/video annotation tool. Let’s start with the easier bit i.e. complete name of tool - Computer Vision Annotation Tool . Ease of Setup . A treat awaits in this section if you have worked with docker compose before, CVAT is very easy to setup as the source code contains necessary docker compose files which makes complete setup a breeze in your local. Repo also features an instruction documentation which is comprehensively written and covers every line of code that needs to be executed in order to get the tool running. . Complete setup is nicely segregated into docker-compose files with docker apps named cvat,cvat_ui, cvat_db. There are two ways to go ahead with setting up the tool, the easier way of deploying it in a VM and a little longer way on Kubernetes. A VM with enough firepower to run an instance of Postgres, Django backend and a react app to support the number of users that you expect to access the tool simultaneously should be alright. An example: I was able to support 200+ users(some of them were automated scripts) in a single VM with the analytics component running using a Standard D5 v2 (16 vcpus, 56 GiB memory) Azure VM. The power of your VM needs to be proportionately increased if you wish to use the additional components like deep learning model based auto labelling. If you choose to deploy the tool in Kubernetes, you could benefit from the auto-scaling functionality for each app inside CVAT. Kubernetes YAML files are not available as part of source code and hence you might need to create them yourself. I recommend Kompose to create Kubrnetes YAML from docker compose files. Scalability is an important aspect while considering a tool for production deployment. In order to make sure that the tool works for foreseeable growth in number of users. App can be scaled easily since everything is nicely wrapped into docker containers. . Ease of Usage . The application’s desktop UI although not fancy, is very feature rich and achieves the goal of labeling images and videos with ease.There’s a long list of keyboard shortcuts supported and you don’t necessarily need to remember every shortcut, simply pick and choose which help you speed up. I found the shortcut to create poly shapes and rectangles during labelling as very useful. Keyboard shortcuts combined with the feature rich app make labelling task slick and smooth. The application also features a task assignment and task process flow using which larger teams can collaborate by assigning tasks to a specific user and updating the current status of task for others to see. CVAT is developed considering the desktop based user interface, which means we need to keep expectations lower while trying to use it on mobile or tablets. You can try the app online right now by navigating to cvat.org CVAT also features a command line interface which enables you to perform simple CRUD operations on task. . Data Extraction/ Upload . A Django app in backend and react UI as frontend, CVAT is quite covered with options to upload data via UI or CLI. App works on a unique task based system, each upload is created as a task in the system. The task can then be further assigned to different users for labelling, quality check, etc. When using the UI to upload data, you don’t need to worry about label formatting because the app takes care of it. CLI based upload requires data to be structured in specific formats before it can be processed. I recommend CLI to perform automated scripts based data upload and UI when actual human is performing the uplaod and labelling task. . Extraction of data is a very important step which will have to be performed by every team on regular basis. CVAT supports extraction of data in a format by both interfaces i.e. UI and CLI. A user can go to a task and there’s option to extract the data in various supported formats. when trying to extract multiple data points or tasks in CVAT, UI based extraction might seem time consuming . CLI comes to rescue here, extraction of data in any supported format is super simple using CLI. An important thing to note here is CVAT currently doesn’t support bulk extraction or upload of data using UI. . The dev team also mentiond about datumaro dataset framework which can be used to transform, merge, extract multiple datasets from CVAT. I was not able to get it working and therefore no comments on that. . Annotations Format Supported . I am borrowing a table available in CVAT documentation to show the formats supported. It supports all major community defined data label formats. The labels covers the spectrum of classification, obejct detection and segmentation tasks in computer vision. . Annotation format Import Export . CVAT for images | X | X | . CVAT for a video | X | X | . Datumaro |   | X | . PASCAL VOC | X | X | . Segmentation masks from PASCAL VOC | X | X | . YOLO | X | X | . MS COCO Object Detection | X | X | . TFrecord | X | X | . MOT | X | X | . LabelMe 3.0 | X | X | . Backup and Restore . All images/videos uploaded are stored in docker volume cvat_data and the respective label data is stored in postgres. Postgres data is stored in docker volume cvat_db. In order to backup the complete app data, you can simply create volume backup for these two volumes in form of .tar files. Configuring the said docker volumes to a persistent storage like S3 or azure blob would enable you to setup automated cloud backup for these volumes. Restoration is as simple as backup by using docker commands. . Community . CVAT community is available on GitHub and Gitter. I have personally found them responding faster on gitter compared to raising issues on github. .",
            "url": "https://piyush.dev/annotation/2020/07/11/CVAT-All-you-need-to-know.html",
            "relUrl": "/annotation/2020/07/11/CVAT-All-you-need-to-know.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Deployment Journey of a Reinforcement Learning Algorithm",
            "content": "Introduction . Our team is working to improve the health and wealth of millions of current customers and acquire more customers in the future. One of the most effective and efficient way to achieve our goal is by getting an app into the millions of people. As it turns out, we already have a wonderful application which is downloaded by more than 3 million users as I write this post. The mobile application has a carousel portion in the bottom half section of the home page where dynamic banners can be rendered. Each banner is utilized as a form of information, communication medium or an application feature. This is the first page that is seen by all users who successfully register and a portion of them clicking on the banner displayed registering their interest. Our team&#39;s goal is to increase engagement within the app. The first step was to understand the source of users who were clicking the banners, why are they willing to go into exploring app via banners after registering while others would go on to explore the app via other routes. . Problem statement . The goal was to increase user engagement within the app by understanding user’s interest in a variety of banners and then leverage the results across the app. . we didn’t have existing data about user interaction with the app neither did we have enough time at hand to perform that activity. We were also looking at an incoming huge inflow of new users expected in near future due to the planned marketing campaigns. We were essentially looking at a cold start problem to improve engagement since, we would know little about the new users and time to market was a very important factor. We were expected to go live within two weeks duration with a solution to make the best out of data available at hand. . Solution . Bayesian bandits with Thomson sampling ticked all boxes as follows: . It requires no data or less to start with compared to other options | It will learn incoming users/data and start recommending banners | Can work with new banners configured as new arms | The next phase of the project was also discussed where we agreed to work on building contextual bandits. In this post, I will be talking more about how we used various tools and technology making deployment possible. I will not be talking about how the recommendation algorithm works and the technology stack used to achieve it. . Deployment . . The build and deployment part of the project was broken down into two technical stages/phases: . Testing model and documenting results in the pre-prod environment with production data, define the input and the output schema for the model which will be used by the data engineer team to create a streaming pipeline. | Setup model to consume a live stream of event data, and respond via a REST endpoint with the recommended list of banners for the users | The front end of the mobile app is configured for a response time of one second w.r.t to back-end. It meant that the app will try to generate dynamic banners on the user screen based on our recommendations or fall back to static banners if we failed to deliver a response within a second, which added another layer of complexity to the second stage. Our APIs were expected to support a wide range of user load starting from a few hundred requests to millions across the region. . We could list the deployment infra into three major components: . A robust build and deployment pipeline | Automated performance testing | Production monitoring and alerting | Tools . Tools used for the complete setup: . Jenkins | Artifactory | Docker | Aquasec image scanning | Fortify static code scan 6.. Sonar Nexus open source code scanning | Kubernetes | Predator | Prometheus | Grafana | Bitbucket | Our application solution is a bunch of docker images which consumes/produces content in Kafka topics. . Step 1 &#8212; Fetching code and checking for changes . Our pipeline starts at fetching the code from Bitbucket repository. We store code in the folder structure for the 4 different docker images that are to be built. We check whether a file has been changed before initiating build for the files in that folder. The code in the Jenkins pipeline is as below for one of the folders titled ‘generator’. . #hide_output script{ GIT_RESULT = sh(script: &#39;&#39;&#39;git diff --quiet HEAD &quot;$(git rev-parse @~1)&quot; -- generator&#39;&#39;&#39;, returnStatus: true ) echo &quot;GIT RESULT -- ${GIT_RESULT} -- ${params.branchname}&quot; } . Step 2 - Fortify . Next step is to run complete code static security scanning by Fortify . #hide_output sh &#39;&#39;&#39; echo &quot;==================================================&quot; echo &quot;======== SAST - Fortify Scan: Start ========&quot; echo &quot;==================================================&quot; hostname whoami ls -ahl echo &#39;WORKSPACE: &#39; $WORKSPACE cd $WORKSPACE pwd sourceanalyzer -v sourceanalyzer -b ${fortify_app_name} -clean sourceanalyzer -b ${fortify_app_name} -python-version ${python_version} -python-path ${python_path} ${fortify_scan_files} sourceanalyzer -b ${fortify_app_name} -scan -f ${fortify_app_name}.fpr fortifyclient -url https://sast.intranet.asia/ssc -authtoken &quot;${fortify_upload_token}&quot; uploadFPR -file ${fortify_app_name}.fpr -project ${fortify_app_name} -version ${fortify_app_version} &#39;&#39;&#39; . Step 3 - Docker . The next step is to build the docker image. We first login to Artifactory before initiating the build as our pip libraries are also pulled from mirrored pip in the Artifactory. I have provided a sample of code on how we achieve this. . #hide_output sh &quot;&quot;&quot; echo ${ARTIFACTORY_PASSWORD} | docker login -u ${ARTIFACTORY_USERNAME} --password-stdin docker-registry:8443 cd generator docker build --file Docker-dev --build-arg HTTPS_PROXY=http://ip-address --build-arg ARTIFACTORY_USERNAME=${ARTIFACTORY_USERNAME} --build-arg ARTIFACTORY_PASSWORD=${ARTIFACTORY_PASSWORD} -t ${env.generator_image_latest} . docker tag ${env.generator_image_latest} ${env.generator_image_name} docker push ${env.generator_image_latest} docker push ${env.generator_image_name} docker logout docker-pcaaicoe.pruregistry.intranet.asia:8443 cd .. &quot;&quot;&quot; . Step 4 - Aquasec . After pushing an image into Artifactory, the next important and mandatory step to be performed is docker image security scanning. . #hide_output sh &quot;&quot;&quot; echo &quot;==================================================&quot; echo &quot;============= OSS - Nexus Scan =============&quot; echo &quot;==================================================&quot; docker save -o generator-dev.tar ${env.generator_image_latest} &quot;&quot;&quot; String result = nexusscan(&quot;pcaaicoeaipulsenudgesgeneratordev&quot;, &quot;$WORKSPACE&quot;, &quot;build&quot;); echo result; sh &quot;&quot;&quot; rm -f generator-dev.tar &quot;&quot;&quot; sh &quot;&quot;&quot; echo &quot;==================================================&quot; echo &quot;============= CSEC - Aquasec Scan ==========&quot; echo &quot;==================================================&quot; &quot;&quot;&quot; aquasecscan(&quot;${env.generator_image_latest}&quot;) . The code and image security scanning stages are major milestones to be cleared during the deployment phase. It is important as well as difficult to explain and agree between application security teams about what risks are we willing to take while allowing open source libraries with bugs to go live in our environment. . Step 5 &#8212; Kubernetes . Now we move on to the stage where we will be able to actually deploy and run our images. In order to deploy our solution, we need a Redis DB and Kafka cluster up and running. We deploy our docker images using the below code: . #hide_output sh &#39;&#39;&#39; set +x echo &quot;- preparing options -&quot; export HTTPS_PROXY=ip-address:8080 export KUBE_NAMESPACE=&quot;internal-namespace&quot; export KC_OPTS=${KC_OPTS}&quot; --kubeconfig=${KUBE_CONFIG}&quot; export KC_OPTS=${KC_OPTS}&quot; --insecure-skip-tls-verify=true&quot; export KC_OPTS=${KC_OPTS}&quot; --namespace=${KUBE_NAMESPACE}&quot; echo &quot;- prepared options -&quot; echo &quot;- preparing alias -&quot; alias kc=&quot;kubectl ${KC_OPTS} $*&quot; echo &quot;- alias prepared -&quot; echo &quot;- applying manifest -&quot; kc apply -f configmap.yaml if [ $which_app = &quot;generator&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-events-reader||echo fi sed -i &quot;s!GENERATOR_VERSION!$image_version!g&quot; &quot;generator.yaml&quot; kc apply -f generator.yaml fi if [ $which_app = &quot;aggregator&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-click-counter||echo fi sed -i &quot;s!AGGREGATOR_VERSION!$image_version!g&quot; &quot;aggregator.yaml&quot; kc apply -f aggregator.yaml fi if [ $which_app = &quot;detector&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-engine||echo fi sed -i &quot;s!DETECTOR_VERSION!$image_version!g&quot; &quot;detector.yaml&quot; kc apply -f detector.yaml fi if [ $which_app = &quot;restapi&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-restapi||echo fi sed -i &quot;s!REST_VERSION!$image_version!g&quot; &quot;restapi.yaml&quot; kc apply -f restapi.yaml fi if [ $which_app = &quot;all&quot; ];then if [ $image_version = &quot;latest&quot; ];then kc delete deploy ai-pulse-nudges-events-reader||echo kc delete deploy ai-pulse-nudges-click-counter||echo kc delete deploy ai-pulse-nudges-engine||echo kc delete deploy ai-pulse-nudges-restapi||echo fi sed -i &quot;s!GENERATOR_VERSION!$image_version!g&quot; &quot;generator.yaml&quot; sed -i &quot;s!AGGREGATOR_VERSION!$image_version!g&quot; &quot;aggregator.yaml&quot; sed -i &quot;s!DETECTOR_VERSION!$image_version!g&quot; &quot;detector.yaml&quot; sed -i &quot;s!REST_VERSION!$image_version!g&quot; &quot;restapi.yaml&quot; kc apply -f generator.yaml kc apply -f aggregator.yaml kc apply -f detector.yaml kc apply -f restapi.yaml fi echo &quot;- manifest applied -&quot; echo &quot;- checking result -&quot; echo &quot; &gt;&gt; Deployments &quot; kc get deployments echo &quot; &gt;&gt; Services&quot; kc get svc echo &quot; &gt;&gt; Ingress&quot; kc get ingress echo &quot; &gt;&gt; Pods&quot; kc get pods echo &quot;- Done -&quot; &#39;&#39;&#39; . Step 6 &#8212; Performance test . We deploy Predator — the tool which we use for performance test. . #hide_output sh &#39;&#39;&#39; set +x echo &quot;- preparing options -&quot; export HTTPS_PROXY=ip-address:8080 export KUBE_NAMESPACE=&quot;internal-namespace&quot; export KC_OPTS=${KC_OPTS}&quot; --kubeconfig=${KUBE_CONFIG}&quot; export KC_OPTS=${KC_OPTS}&quot; --insecure-skip-tls-verify=true&quot; export KC_OPTS=${KC_OPTS}&quot; --namespace=${KUBE_NAMESPACE}&quot; echo &quot;- prepared options -&quot; echo &quot;- preparing alias -&quot; alias kc=&quot;kubectl ${KC_OPTS} $*&quot; echo &quot;- alias prepared -&quot; echo &quot;- applying manifest -&quot; kc get deploy|grep predator|awk &#39;{print $1 }&#39; || echo kc get deploy|grep predator|awk &#39;{print $1 }&#39;|xargs kc delete deploy || echo for i in `seq $replica_count` do echo $i cp -rf predator/predator.yaml tmp.yaml sed -i &quot;s!REPLICA_NO!$i&quot;&quot;!g&quot; &quot;tmp.yaml&quot; kc apply -f tmp.yaml done # kc apply -f predator/predator.yaml echo &quot;- manifest applied -&quot; echo &quot;- checking result -&quot; echo &quot; &gt;&gt; Deployments &quot; kc get deployments echo &quot; &gt;&gt; Services&quot; kc get svc echo &quot; &gt;&gt; Ingress&quot; kc get ingress echo &quot; &gt;&gt; Pods&quot; kc get pods echo &quot;- Done -&quot; &#39;&#39;&#39; . Predator is an amazing tool that enables us to leverage existing Kubernetes infra for an unlimited number of users for testing. Read more about the tool here: https://medium.com/zooz-engineering/by-niv-lipetz-software-engineer-zooz-b5928da0b7a8 We leverage the existing enterprise Prometheus and Grafana set up to monitor the application pods. . Lessons learned for next time: . We started writing the pipeline code from scratch, whereas it would have helped save time if an advanced hello world type of empty pipeline existed, which could be used as a template structure. It would have enabled us to know what credentials and access were required at what stage. | There were many credentials and access that were required to get the pipeline up and running. It would be a time and effort savior if we have one master service id created and assigned to a pipeline which can then be used across all tools in the organization. | It is very difficult to build a machine learning model, and real-time streaming data was an additional complexity, but productionizing that model with streaming data is many folds difficult. | Contributors . Glenn Bayne, Tien Nguyet Long, John Yue, Zeldon Tay （郑育忠), Steven Chau , Denys Pang , Philipp Gschoepf , Syam Bandi , Uma Maheshwari, Michael Natusch .",
            "url": "https://piyush.dev/deployment/2020/07/07/Deploying-RL-Model.html",
            "relUrl": "/deployment/2020/07/07/Deploying-RL-Model.html",
            "date": " • Jul 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, Welcome to Piyush.Dev . I am a data scientist who enjoys playing with new technologies and identify possible use cases in the real world. . I am an engineer by qualification, turned to data science by sheer nature of curiosity. . I like learning new things and sharing here. . Welcome to my site. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://piyush.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://piyush.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}